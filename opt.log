Args: /usr/local/python3.11.13/lib/python3.11/site-packages/triton/backends/ascend/triton-adapter-opt input.mlir --discrete-mask-access-conversion --triton-to-annotation --triton-to-hivm --triton-to-unstructure -debug --triton-to-linalg=global-kernel=false named-ops=true -o output.mlir 
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemRefLayoutAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ElementsAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DistinctAttr)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ResourceBlobManagerDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeDialectInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineBinaryOpExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineConstantExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineDimExprStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::AffineMapStorage)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::IntegerSetStorage)
Load new dialect in Context builtin
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneRegion<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroResults<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroSuccessors<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoRegionArguments<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NoTerminator<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlock<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OpInvariants<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BytecodeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AffineScope<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIsolatedFromAbove<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SymbolTable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpAsmOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionKindInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasOnlyGraphRegion<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::AttributeTrait::IsLocation<Empty>)
Load new dialect in Context tt
Load new dialect in Context arith
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithFastMathInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithRoundingModeInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SelectLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectInlinerInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConvertToLLVMPatternInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferDeallocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::BufferizableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueBoundsOpInterface)
Load new dialect in Context math
Load new dialect in Context scf
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ParallelCombiningOpInterface)
Load new dialect in Context cf
ImplicitTypeIDRegistry::lookupOrInsert(mlir::BranchOpInterface)
Load new dialect in Context ub
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ub::PoisonAttrInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SymbolUserOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::DescriptorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::DescriptorStoreLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TensorOrMemDesc)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AutomaticAllocationScope<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CallableOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::FunctionOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::TensorSizeTrait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VerifyTensorLayoutsTrait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ZeroRegions<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneResult<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::Type>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ConstantLike<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ConditionallySpeculatable::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AlwaysSpeculatableImplTrait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffectOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferIntRangeInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::InferTypeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::ConstantOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::IntegerType>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::detail::GetProgramIdOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::NOperands<2>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsCommutative<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::ArithIntegerOverflowFlagsInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultType<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::VectorUnrollOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Elementwise<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Scalarizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Vectorizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::Tensorizable<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneTypedResult<mlir::RankedTensorType>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::OneOperand<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultElementType<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultEncoding<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameTypeOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::CmpIOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicResults<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<3>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SingleBlockImplicitTerminator<mlir::scf::YieldOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::LoopLikeOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasRecursiveMemoryEffects<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameOperandsAndResultShape<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<1>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AttrSizedOperandSegments<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsAndResultShape<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsAndResultEncoding<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::detail::LoadOpGenericAdaptorBase::Properties)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsIdempotent<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::AtLeastNOperands<2>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsShape<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::SameLoadStoreOperandsEncoding<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::IsTerminator<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::VariadicOperands<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::scf::ExecuteRegionOp, mlir::scf::ForOp, mlir::scf::IfOp, mlir::scf::IndexSwitchOp, mlir::scf::WhileOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RegionBranchTerminatorOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::ReturnLike<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OpTrait::HasParent<mlir::triton::FuncOp>::Impl<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ValueSemantics<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedType::Trait<Empty>)
Load new dialect in Context affine
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaStartOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineMapAccessInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineDmaWaitOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineReadOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::affine::AffineWriteOpInterface)
Load new dialect in Context annotation
Load new dialect in Context bufferization
Load new dialect in Context memref
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CopyOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableMemOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAccessorOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PromotableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableAllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ViewLikeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ShapedDimOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OffsetSizeAndStrideOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::bufferization::AllocationOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::RuntimeVerifiableOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestructurableTypeInterface)
Load new dialect in Context tensor
Load new dialect in Context complex
ImplicitTypeIDRegistry::lookupOrInsert(mlir::transform::FindPayloadReplacementOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetInsertionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::SubsetExtractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TilingInterface)
Load new dialect in Context func
Load new dialect in Context hivm
Load new dialect in Context linalg
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::AggregatedOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::LinalgOp)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ContractionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::ConvolutionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::linalg::FillOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::mesh::ShardingInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::PartialReductionOpInterface)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::OpToOpPassAdaptor)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DialectFoldInterface)

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x5e9083650000) {
  "tt.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x5e9083646c30) {
  "scf.yield"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x5e9083664460) {
  "tt.store"(%39, %34, %33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xf32>, tensor<1024x16xi1>) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Write)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::triton::GlobalMemory)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Allocate)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::MemoryEffects::Read)

  * Pattern DiscreteMaskStoreConversion : 'tt.store -> ()' {
Trying to match "DiscreteMaskStoreConversion"
[MaskState]==> parse op
%26 = arith.andi %25, %12 : tensor<1024x16xi1>
[MaskState]<==
[MaskState]==> parse op
%25 = tt.broadcast %24 : tensor<1024x1xi1> -> tensor<1024x16xi1>
[MaskState]<==
[MaskState]==> parse op
%24 = tt.expand_dims %16 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
[MaskState]<==
[MaskState]==> parse op
%16 = arith.cmpi slt, %15, %cst_2 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%15 = arith.addi %4, %14 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%4 = arith.addi %2, %3 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%3 = tt.splat %1 : i32 -> tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.index_cast'(0x5e90836a69d0)
    ** Insert  : 'arith.constant'(0x5e90836acf40)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::AddIOpGenericAdaptorBase::Properties)
    ** Insert  : 'arith.addi'(0x5e908365f530)
[MaskState]==> parse op
%14 = tt.splat %arg4 : i32 -> tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.index_cast'(0x5e90836ac3f0)
    ** Insert  : 'arith.addi'(0x5e908365f5e0)
    ** Insert  : 'arith.addi'(0x5e90836accc0)
[MaskState]==> parse op
%cst_2 = arith.constant dense<1971940> : tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.constant'(0x5e90836acfb0)
    ** Insert  : 'arith.maxsi'(0x5e90836ada00)
    ** Insert  : 'arith.minsi'(0x5e90836adab0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::arith::detail::SubIOpGenericAdaptorBase::Properties)
    ** Insert  : 'arith.subi'(0x5e90836adb60)
[MaskState]==> parse op
%12 = tt.broadcast %11 : tensor<1x16xi1> -> tensor<1024x16xi1>
[MaskState]<==
[MaskState]==> parse op
%11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi1> -> tensor<1x16xi1>
[MaskState]<==
[MaskState]==> parse op
%7 = arith.cmpi slt, %6, %cst_0 : tensor<16xi32>
[MaskState]<==
[MaskState]==> parse op
%6 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
[MaskState]<==
[MaskState]==> parse op
%cst_0 = arith.constant dense<16> : tensor<16xi32>
[MaskState]<==
    ** Insert  : 'arith.constant'(0x5e90836af510)
    ** Insert  : 'arith.minsi'(0x5e90836af580)
[MaskState]==> inserted op: 
%41 = arith.minsi %40, %c1024_3 : index
[MaskState]<== is removed
    ** Erase   : 'arith.minsi'(0x5e90836af580)
[MaskState]==> inserted op: 
%c1024_3 = arith.constant 1024 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836af510)
[MaskState]==> inserted op: 
%40 = arith.subi %39, %36 : index
[MaskState]<== is removed
    ** Erase   : 'arith.subi'(0x5e90836adb60)
[MaskState]==> inserted op: 
%39 = arith.minsi %37, %38 : index
[MaskState]<== is removed
    ** Erase   : 'arith.minsi'(0x5e90836adab0)
[MaskState]==> inserted op: 
%38 = arith.maxsi %36, %c1971940 : index
[MaskState]<== is removed
    ** Erase   : 'arith.maxsi'(0x5e90836ada00)
[MaskState]==> inserted op: 
%c1971940 = arith.constant 1971940 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836acfb0)
[MaskState]==> inserted op: 
%36 = arith.addi %33, %35 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e908365f5e0)
[MaskState]==> inserted op: 
%36 = arith.addi %34, %35 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e90836accc0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::CastOpInterface::Trait<Empty>)
[MaskState]==> inserted op: 
%35 = arith.index_cast %arg4 : i32 to index
[MaskState]<== is removed
    ** Erase   : 'arith.index_cast'(0x5e90836ac3f0)
[MaskState]==> inserted op: 
%34 = arith.addi %c1024, %33 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e908365f530)
[MaskState]==> inserted op: 
%33 = arith.index_cast %1 : i32 to index
[MaskState]<== is removed
    ** Erase   : 'arith.index_cast'(0x5e90836a69d0)
[MaskState]==> inserted op: 
%c1024 = arith.constant 1024 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836acf40)
"DiscreteMaskStoreConversion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cf40) {
  %39 = "tt.addptr"(%38, %17) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366ce50) {
  %38 = "tt.broadcast"(%37) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cd40) {
  %37 = "tt.addptr"(%20, %36) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366cc30) {
  %36 = "arith.muli"(%35, %2) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366cb40) {
  %35 = "tt.expand_dims"(%22) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e908365f8d0) {
  %34 = "tt.load"(%30, %33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 0>}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi1>) -> tensor<1024x16xf32>


  * Pattern DiscreteMaskLoadConversion : 'tt.load -> ()' {
Trying to match "DiscreteMaskLoadConversion"
[MaskState]==> parse op
%26 = arith.andi %25, %12 : tensor<1024x16xi1>
[MaskState]<==
[MaskState]==> parse op
%25 = tt.broadcast %24 : tensor<1024x1xi1> -> tensor<1024x16xi1>
[MaskState]<==
[MaskState]==> parse op
%24 = tt.expand_dims %16 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
[MaskState]<==
[MaskState]==> parse op
%16 = arith.cmpi slt, %15, %cst_2 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%15 = arith.addi %4, %14 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%4 = arith.addi %2, %3 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%3 = tt.splat %1 : i32 -> tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.index_cast'(0x5e90836a69d0)
    ** Insert  : 'arith.constant'(0x5e90836acfb0)
    ** Insert  : 'arith.addi'(0x5e908365f530)
[MaskState]==> parse op
%14 = tt.splat %arg4 : i32 -> tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.index_cast'(0x5e90836ac3f0)
    ** Insert  : 'arith.addi'(0x5e90836accc0)
    ** Insert  : 'arith.addi'(0x5e908365f5e0)
[MaskState]==> parse op
%cst_2 = arith.constant dense<1971940> : tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.constant'(0x5e90836af510)
    ** Insert  : 'arith.maxsi'(0x5e90836ada00)
    ** Insert  : 'arith.minsi'(0x5e90836adab0)
    ** Insert  : 'arith.subi'(0x5e90836adb60)
[MaskState]==> parse op
%12 = tt.broadcast %11 : tensor<1x16xi1> -> tensor<1024x16xi1>
[MaskState]<==
[MaskState]==> parse op
%11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi1> -> tensor<1x16xi1>
[MaskState]<==
[MaskState]==> parse op
%7 = arith.cmpi slt, %6, %cst_0 : tensor<16xi32>
[MaskState]<==
[MaskState]==> parse op
%6 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
[MaskState]<==
[MaskState]==> parse op
%cst_0 = arith.constant dense<16> : tensor<16xi32>
[MaskState]<==
    ** Insert  : 'arith.constant'(0x5e90836aef70)
    ** Insert  : 'arith.minsi'(0x5e90836af020)
[MaskState]==> inserted op: 
%35 = arith.minsi %34, %c1024_3 : index
[MaskState]<== is removed
    ** Erase   : 'arith.minsi'(0x5e90836af020)
[MaskState]==> inserted op: 
%c1024_3 = arith.constant 1024 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836aef70)
[MaskState]==> inserted op: 
%34 = arith.subi %33, %30 : index
[MaskState]<== is removed
    ** Erase   : 'arith.subi'(0x5e90836adb60)
[MaskState]==> inserted op: 
%33 = arith.minsi %31, %32 : index
[MaskState]<== is removed
    ** Erase   : 'arith.minsi'(0x5e90836adab0)
[MaskState]==> inserted op: 
%32 = arith.maxsi %30, %c1971940 : index
[MaskState]<== is removed
    ** Erase   : 'arith.maxsi'(0x5e90836ada00)
[MaskState]==> inserted op: 
%c1971940 = arith.constant 1971940 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836af510)
[MaskState]==> inserted op: 
%30 = arith.addi %27, %29 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e90836accc0)
[MaskState]==> inserted op: 
%30 = arith.addi %28, %29 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e908365f5e0)
[MaskState]==> inserted op: 
%29 = arith.index_cast %arg4 : i32 to index
[MaskState]<== is removed
    ** Erase   : 'arith.index_cast'(0x5e90836ac3f0)
[MaskState]==> inserted op: 
%28 = arith.addi %c1024, %27 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e908365f530)
[MaskState]==> inserted op: 
%27 = arith.index_cast %1 : i32 to index
[MaskState]<== is removed
    ** Erase   : 'arith.index_cast'(0x5e90836a69d0)
[MaskState]==> inserted op: 
%c1024 = arith.constant 1024 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836acfb0)
"DiscreteMaskLoadConversion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x5e9083669fb0) {
  %33 = "arith.andi"(%32, %19) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366c200) {
  %32 = "tt.broadcast"(%31) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366c110) {
  %31 = "tt.expand_dims"(%23) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366c000) {
  %30 = "tt.addptr"(%29, %17) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366bf10) {
  %29 = "tt.broadcast"(%28) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366be00) {
  %28 = "tt.addptr"(%15, %27) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366bcf0) {
  %27 = "arith.muli"(%26, %2) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366b780) {
  %26 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e90836419a0) {
  %25 = "tt.load"(%24, %23, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>


  * Pattern DiscreteMaskLoadConversion : 'tt.load -> ()' {
Trying to match "DiscreteMaskLoadConversion"
[MaskState]==> parse op
%16 = arith.cmpi slt, %15, %cst_2 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%15 = arith.addi %4, %14 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%4 = arith.addi %2, %3 : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
[MaskState]<==
[MaskState]==> parse op
%3 = tt.splat %1 : i32 -> tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.index_cast'(0x5e90836a69d0)
    ** Insert  : 'arith.constant'(0x5e90836acfb0)
    ** Insert  : 'arith.addi'(0x5e908365f5e0)
[MaskState]==> parse op
%14 = tt.splat %arg4 : i32 -> tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.index_cast'(0x5e90836ac3f0)
    ** Insert  : 'arith.addi'(0x5e90836accc0)
    ** Insert  : 'arith.addi'(0x5e90836ada00)
[MaskState]==> parse op
%cst_2 = arith.constant dense<1971940> : tensor<1024xi32>
[MaskState]<==
    ** Insert  : 'arith.constant'(0x5e90836af510)
    ** Insert  : 'arith.maxsi'(0x5e90836adab0)
    ** Insert  : 'arith.minsi'(0x5e90836adb60)
    ** Insert  : 'arith.subi'(0x5e90836af020)
[MaskState]==> inserted op: 
%25 = arith.subi %24, %21 : index
[MaskState]<== is removed
    ** Erase   : 'arith.subi'(0x5e90836af020)
[MaskState]==> inserted op: 
%24 = arith.minsi %22, %23 : index
[MaskState]<== is removed
    ** Erase   : 'arith.minsi'(0x5e90836adb60)
[MaskState]==> inserted op: 
%23 = arith.maxsi %21, %c1971940 : index
[MaskState]<== is removed
    ** Erase   : 'arith.maxsi'(0x5e90836adab0)
[MaskState]==> inserted op: 
%c1971940 = arith.constant 1971940 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836af510)
[MaskState]==> inserted op: 
%21 = arith.addi %18, %20 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e90836accc0)
[MaskState]==> inserted op: 
%21 = arith.addi %19, %20 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e90836ada00)
[MaskState]==> inserted op: 
%20 = arith.index_cast %arg4 : i32 to index
[MaskState]<== is removed
    ** Erase   : 'arith.index_cast'(0x5e90836ac3f0)
[MaskState]==> inserted op: 
%19 = arith.addi %c1024, %18 : index
[MaskState]<== is removed
    ** Erase   : 'arith.addi'(0x5e908365f5e0)
[MaskState]==> inserted op: 
%18 = arith.index_cast %1 : i32 to index
[MaskState]<== is removed
    ** Erase   : 'arith.index_cast'(0x5e90836a69d0)
[MaskState]==> inserted op: 
%c1024 = arith.constant 1024 : index
[MaskState]<== is removed
    ** Erase   : 'arith.constant'(0x5e90836acfb0)
"DiscreteMaskLoadConversion" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366ad10) {
  %24 = "tt.addptr"(%12, %22) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e908366ac20) {
  %23 = "arith.cmpi"(%22, %5) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e908366a670) {
  %22 = "arith.addi"(%11, %21) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e908366d040) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e908366a100) {
  %21 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083669330) {
  %20 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e9083668dc0) {
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083668cd0) {
  %18 = "tt.expand_dims"(%14) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836687a0) {
  %17 = "tt.broadcast"(%16) : (tensor<1x16xi32>) -> tensor<1024x16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083649170) {
  %16 = "tt.expand_dims"(%13) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083648c00) {
  %15 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e9083648b10) {
  %14 = "arith.cmpi"(%13, %3) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e9083666b90) {
  %13 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083666aa0) {
  %12 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e9083666510) {
  %11 = "arith.addi"(%9, %10) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083665b20) {
  %10 = "tt.splat"(%8) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e90836655d0) {
  %9 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e90836646f0) {
  %8 = "arith.muli"(%7, %6) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5e9083653f20) {
  %7 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836645e0) {
  %6 = "arith.constant"() <{value = 49299 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662c20) {
  %5 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083664400) {
  %4 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836635b0) {
  %3 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662730) {
  %2 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836613f0) {
  %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x5e908366d1b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083660580) {
  %0 = "arith.constant"() <{value = 1024 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::detail::PreservedAnalyses::AllAnalysesType)

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x5e908366d2c0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.func'(0x5e908366d1b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083660580) {
  %0 = "arith.constant"() <{value = 1024 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e90836613f0) {
  %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083662730) {
  %2 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e90836635b0) {
  %3 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083664400) {
  %4 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083662c20) {
  %5 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e90836645e0) {
  %6 = "arith.constant"() <{value = 49299 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.get_program_id'(0x5e9083653f20) {
  %7 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x5e90836646f0) {
  %8 = "arith.muli"(%7, %6) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.make_range'(0x5e90836655d0) {
  %9 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083665b20) {
  %10 = "tt.splat"(%8) : (i32) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x5e9083666510) {
  %11 = "arith.addi"(%9, %10) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083666aa0) {
  %12 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.make_range'(0x5e9083666b90) {
  %13 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x5e9083648b10) {
  %14 = "arith.cmpi"(%13, %3) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083648c00) {
  %15 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e9083649170) {
  %16 = "tt.expand_dims"(%13) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e90836687a0) {
  %17 = "tt.broadcast"(%16) : (tensor<1x16xi32>) -> tensor<1024x16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e9083668cd0) {
  %18 = "tt.expand_dims"(%14) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e9083668dc0) {
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi1>) -> tensor<1024x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083669330) {
  %20 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.for'(0x5e908366d040) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e908366a100) {
  %21 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x5e908366a670) {
  %22 = "arith.addi"(%11, %21) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x5e908366ac20) {
  %23 = "arith.cmpi"(%22, %5) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366ad10) {
  %24 = "tt.addptr"(%12, %22) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.load'(0x5e90836419a0) {
  %25 = "tt.load"(%24, %23, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e908366b780) {
  %26 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x5e908366bcf0) {
  %27 = "arith.muli"(%26, %2) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366be00) {
  %28 = "tt.addptr"(%15, %27) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e908366bf10) {
  %29 = "tt.broadcast"(%28) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366c000) {
  %30 = "tt.addptr"(%29, %17) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e908366c110) {
  %31 = "tt.expand_dims"(%23) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e908366c200) {
  %32 = "tt.broadcast"(%31) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.andi'(0x5e9083669fb0) {
  %33 = "arith.andi"(%32, %19) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.load'(0x5e908365f8d0) {
  %34 = "tt.load"(%30, %33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 0>}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi1>) -> tensor<1024x16xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e908366cb40) {
  %35 = "tt.expand_dims"(%22) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x5e908366cc30) {
  %36 = "arith.muli"(%35, %2) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366cd40) {
  %37 = "tt.addptr"(%20, %36) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e908366ce50) {
  %38 = "tt.broadcast"(%37) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366cf40) {
  %39 = "tt.addptr"(%38, %17) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.store'(0x5e9083664460) {
  "tt.store"(%39, %34, %33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xf32>, tensor<1024x16xi1>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x5e9083646c30) {
  "scf.yield"() : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.return'(0x5e9083650000) {
  "tt.return"() : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'builtin.module'(0x5e908366d2c0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.func'(0x5e908366d1b0) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083660580) {
  %0 = "arith.constant"() <{value = 1024 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e90836613f0) {
  %1 = "arith.constant"() <{value = 0 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083662730) {
  %2 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e90836635b0) {
  %3 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083664400) {
  %4 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e9083662c20) {
  %5 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.constant'(0x5e90836645e0) {
  %6 = "arith.constant"() <{value = 49299 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.get_program_id'(0x5e9083653f20) {
  %7 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x5e90836646f0) {
  %8 = "arith.muli"(%7, %6) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.make_range'(0x5e90836655d0) {
  %9 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083665b20) {
  %10 = "tt.splat"(%8) : (i32) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x5e9083666510) {
  %11 = "arith.addi"(%9, %10) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083666aa0) {
  %12 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.make_range'(0x5e9083666b90) {
  %13 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x5e9083648b10) {
  %14 = "arith.cmpi"(%13, %3) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083648c00) {
  %15 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e9083649170) {
  %16 = "tt.expand_dims"(%13) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e90836687a0) {
  %17 = "tt.broadcast"(%16) : (tensor<1x16xi32>) -> tensor<1024x16xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e9083668cd0) {
  %18 = "tt.expand_dims"(%14) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e9083668dc0) {
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi1>) -> tensor<1024x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e9083669330) {
  %20 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.for'(0x5e908366d040) {
  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.splat'(0x5e908366a100) {
  %21 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.addi'(0x5e908366a670) {
  %22 = "arith.addi"(%11, %21) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.cmpi'(0x5e908366ac20) {
  %23 = "arith.cmpi"(%22, %5) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366ad10) {
  %24 = "tt.addptr"(%12, %22) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.load'(0x5e90836419a0) {
  %25 = "tt.load"(%24, %23, %4) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e908366b780) {
  %26 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x5e908366bcf0) {
  %27 = "arith.muli"(%26, %2) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366be00) {
  %28 = "tt.addptr"(%15, %27) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e908366bf10) {
  %29 = "tt.broadcast"(%28) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366c000) {
  %30 = "tt.addptr"(%29, %17) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e908366c110) {
  %31 = "tt.expand_dims"(%23) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e908366c200) {
  %32 = "tt.broadcast"(%31) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.andi'(0x5e9083669fb0) {
  %33 = "arith.andi"(%32, %19) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.load'(0x5e908365f8d0) {
  %34 = "tt.load"(%30, %33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 0>}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi1>) -> tensor<1024x16xf32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.expand_dims'(0x5e908366cb40) {
  %35 = "tt.expand_dims"(%22) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'arith.muli'(0x5e908366cc30) {
  %36 = "arith.muli"(%35, %2) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366cd40) {
  %37 = "tt.addptr"(%20, %36) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.broadcast'(0x5e908366ce50) {
  %38 = "tt.broadcast"(%37) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.addptr'(0x5e908366cf40) {
  %39 = "tt.addptr"(%38, %17) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.store'(0x5e9083664460) {
  "tt.store"(%39, %34, %33) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xf32>, tensor<1024x16xi1>) -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'scf.yield'(0x5e9083646c30) {
  "scf.yield"() : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//

//===-------------------------------------------===//
Legalizing operation : 'tt.return'(0x5e9083650000) {
  "tt.return"() : () -> ()

  * Fold {
  } -> FAILURE : unable to fold
} -> FAILURE : no matched legalization pattern
//===-------------------------------------------===//
tt.func public @origin_index_select(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c1024_i32 = arith.constant 1024 : i32
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant dense<16> : tensor<1024x1xi32>
  %cst_0 = arith.constant dense<16> : tensor<16xi32>
  %cst_1 = arith.constant dense<0> : tensor<1024xi32>
  %cst_2 = arith.constant dense<1971940> : tensor<1024xi32>
  %c49299_i32 = arith.constant 49299 : i32
  %0 = tt.get_program_id x : i32
  %1 = arith.muli %0, %c49299_i32 : i32
  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
  %3 = tt.splat %1 : i32 -> tensor<1024xi32>
  %4 = arith.addi %2, %3 : tensor<1024xi32>
  %5 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>>
  %6 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
  %7 = arith.cmpi slt, %6, %cst_0 : tensor<16xi32>
  %8 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  %9 = tt.expand_dims %6 {axis = 0 : i32} : tensor<16xi32> -> tensor<1x16xi32>
  %10 = tt.broadcast %9 : tensor<1x16xi32> -> tensor<1024x16xi32>
  %11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi1> -> tensor<1x16xi1>
  %12 = tt.broadcast %11 : tensor<1x16xi1> -> tensor<1024x16xi1>
  %13 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  scf.for %arg4 = %c0_i32 to %c49299_i32 step %c1024_i32  : i32 {
    %14 = tt.splat %arg4 : i32 -> tensor<1024xi32>
    %15 = arith.addi %4, %14 : tensor<1024xi32>
    %16 = arith.cmpi slt, %15, %cst_2 : tensor<1024xi32>
    %17 = tt.addptr %5, %15 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
    %18 = tt.load %17, %16, %cst_1 : tensor<1024x!tt.ptr<i32>>
    %19 = tt.expand_dims %18 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %20 = arith.muli %19, %cst : tensor<1024x1xi32>
    %21 = tt.addptr %8, %20 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %22 = tt.broadcast %21 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %23 = tt.addptr %22, %10 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    %24 = tt.expand_dims %16 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
    %25 = tt.broadcast %24 : tensor<1024x1xi1> -> tensor<1024x16xi1>
    %26 = arith.andi %25, %12 : tensor<1024x16xi1>
    %27 = tt.load %23, %26 : tensor<1024x16x!tt.ptr<f32>>
    %28 = tt.expand_dims %15 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %29 = arith.muli %28, %cst : tensor<1024x1xi32>
    %30 = tt.addptr %13, %29 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %31 = tt.broadcast %30 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %32 = tt.addptr %31, %10 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    tt.store %32, %27, %26 : tensor<1024x16x!tt.ptr<f32>>
  }
  tt.return
}
========================================================
KMLOG Parsing tt.load %18 = tt.load %17, %16, %cst_1 : tensor<1024x!tt.ptr<i32>>parse: %17 = tt.addptr %5, %15 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
parse: %5 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>>
parse: <block argument> of type '!tt.ptr<i32>' at index: 1
finish parse: <block argument> of type '!tt.ptr<i32>' at index: 1 structured: 
[parseSplat] dst is
%5 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>>
finish parse: %6 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>> structured: 1 
parse: %16 = arith.addi %4, %15 : tensor<1024xi32>
parse: %4 = arith.addi %2, %3 : tensor<1024xi32>
parse: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
finish parse: %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> structured: 1 
parse: %3 = tt.splat %1 : i32 -> tensor<1024xi32>
parse: %1 = arith.muli %0, %c49299_i32 : i32
parse: %0 = tt.get_program_id x : i32
finish parse: %0 = tt.get_program_id x : i32 structured: 
parse: %c49299_i32 = arith.constant 49299 : i32
finish parse: %c49299_i32 = arith.constant 49299 : i32 structured: 
finish parse: %1 = arith.muli %0, %c49299_i32 : i32 structured: 
[parseSplat] dst is
%3 = tt.splat %1 : i32 -> tensor<1024xi32>
finish parse: %3 = tt.splat %1 : i32 -> tensor<1024xi32> structured: 1 
finish parse: %4 = arith.addi %2, %3 : tensor<1024xi32> structured: 1 
parse: %15 = tt.splat %arg4 : i32 -> tensor<1024xi32>
parse: <block argument> of type 'i32' at index: 0
Handling block argument
scf.for %arg4 = %c0_i32 to %c49299_i32 step %c1024_i32  : i32 {
  %15 = tt.splat %arg4 : i32 -> tensor<1024xi32>
  %16 = arith.addi %4, %15 : tensor<1024xi32>
  %17 = arith.cmpi slt, %16, %cst_3 : tensor<1024xi32>
  %18 = tt.addptr %6, %16 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
  %19 = tt.load %18, %17, %cst_2 : tensor<1024x!tt.ptr<i32>>
  %20 = tt.expand_dims %19 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
  %21 = arith.muli %20, %cst : tensor<1024x1xi32>
  %22 = tt.addptr %9, %21 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
  %23 = tt.broadcast %22 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
  %24 = tt.addptr %23, %11 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
  %25 = tt.expand_dims %17 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
  %26 = tt.broadcast %25 : tensor<1024x1xi1> -> tensor<1024x16xi1>
  %27 = arith.andi %26, %13 : tensor<1024x16xi1>
  %28 = tt.load %24, %27 : tensor<1024x16x!tt.ptr<f32>>
  %29 = tt.expand_dims %16 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
  %30 = arith.muli %29, %cst : tensor<1024x1xi32>
  %31 = tt.addptr %14, %30 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
  %32 = tt.broadcast %31 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
  %33 = tt.addptr %32, %11 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
  tt.store %33, %28, %27 : tensor<1024x16x!tt.ptr<f32>>
}
finish parse: <block argument> of type 'i32' at index: 0 structured: 
[parseSplat] dst is
%15 = tt.splat %arg4 : i32 -> tensor<1024xi32>
finish parse: %15 = tt.splat %arg4 : i32 -> tensor<1024xi32> structured: 1 
finish parse: %16 = arith.addi %4, %15 : tensor<1024xi32> structured: 1 
    [parseAddPtr] Adding offset
    %5 = tt.splat %c0_i64_0 : i64 -> tensor<1024xi64>
    %18 = arith.extsi %16 : tensor<1024xi32> to tensor<1024xi64>
    [parseAddPtr] offset is
%19 = arith.addi %5, %18 : tensor<1024xi64>
[parseAddPtr] ptrStructured:     1
    [parseAddPtr] offsetStructured:     1
finish parse: %20 = tt.addptr %6, %16 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32> structured: 1 
KMLOG Parsing done for: %21 = tt.load %20, %17, %cst_2 : tensor<1024x!tt.ptr<i32>>
[  1]
tt.func public @origin_index_select(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %c1024_i32 = arith.constant 1024 : i32
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant dense<16> : tensor<1024x1xi32>
  %cst_1 = arith.constant dense<16> : tensor<16xi32>
  %cst_2 = arith.constant dense<0> : tensor<1024xi32>
  %cst_3 = arith.constant dense<1971940> : tensor<1024xi32>
  %c49299_i32 = arith.constant 49299 : i32
  %0 = tt.get_program_id x : i32
  %1 = arith.muli %0, %c49299_i32 : i32
  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
  %3 = tt.splat %1 : i32 -> tensor<1024xi32>
  %4 = arith.addi %2, %3 : tensor<1024xi32>
  %5 = tt.splat %c0_i64_0 : i64 -> tensor<1024xi64>
  %6 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>>
  %7 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
  %8 = arith.cmpi slt, %7, %cst_1 : tensor<16xi32>
  %9 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  %10 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi32> -> tensor<1x16xi32>
  %11 = tt.broadcast %10 : tensor<1x16xi32> -> tensor<1024x16xi32>
  %12 = tt.expand_dims %8 {axis = 0 : i32} : tensor<16xi1> -> tensor<1x16xi1>
  %13 = tt.broadcast %12 : tensor<1x16xi1> -> tensor<1024x16xi1>
  %14 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  scf.for %arg4 = %c0_i32 to %c49299_i32 step %c1024_i32  : i32 {
    %c0_i64_4 = arith.constant 0 : i64
    %15 = tt.splat %arg4 : i32 -> tensor<1024xi32>
    %16 = arith.addi %4, %15 : tensor<1024xi32>
    %17 = arith.cmpi slt, %16, %cst_3 : tensor<1024xi32>
    %18 = arith.extsi %16 : tensor<1024xi32> to tensor<1024xi64>
    %19 = arith.addi %5, %18 : tensor<1024xi64>
    %20 = tt.addptr %6, %16 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
    %21 = tt.load %20, %17, %cst_2 : tensor<1024x!tt.ptr<i32>>
    %22 = tt.expand_dims %21 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %23 = arith.muli %22, %cst : tensor<1024x1xi32>
    %24 = tt.addptr %9, %23 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %25 = tt.broadcast %24 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %26 = tt.addptr %25, %11 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    %27 = tt.expand_dims %17 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
    %28 = tt.broadcast %27 : tensor<1024x1xi1> -> tensor<1024x16xi1>
    %29 = arith.andi %28, %13 : tensor<1024x16xi1>
    %30 = tt.load %26, %29 : tensor<1024x16x!tt.ptr<f32>>
    %31 = tt.expand_dims %16 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %32 = arith.muli %31, %cst : tensor<1024x1xi32>
    %33 = tt.addptr %14, %32 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %34 = tt.broadcast %33 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %35 = tt.addptr %34, %11 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    tt.store %35, %30, %29 : tensor<1024x16x!tt.ptr<f32>>
  }
  tt.return
}
========================================================
KMLOG Parsing tt.load %30 = tt.load %26, %29 : tensor<1024x16x!tt.ptr<f32>>parse: %26 = tt.addptr %25, %11 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
parse: %25 = tt.broadcast %24 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
parse: %24 = tt.addptr %9, %23 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
parse: %9 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
parse: <block argument> of type '!tt.ptr<f32>' at index: 0
finish parse: <block argument> of type '!tt.ptr<f32>' at index: 0 structured: 
[parseSplat] dst is
%9 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
finish parse: %10 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>> structured: 1 1 
parse: %24 = arith.muli %23, %cst : tensor<1024x1xi32>
parse: %23 = tt.expand_dims %22 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
parse: %22 = tt.load %21, %18, %cst_4 : tensor<1024x!tt.ptr<i32>>
found: %21 = tt.addptr %6, %17 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
finish parse: %22 = tt.load %21, %18, %cst_4 : tensor<1024x!tt.ptr<i32>> structured: 0 
finish parse: %23 = tt.expand_dims %22 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32> structured: 0 1 
parse: %cst = arith.constant dense<16> : tensor<1024x1xi32>
finish parse: %cst = arith.constant dense<16> : tensor<1024x1xi32> structured: 1 1 
finish parse: %24 = arith.muli %23, %cst : tensor<1024x1xi32> structured: 0 1 
    [parseAddPtr] Adding offset
    %9 = tt.splat %c0_i64_0 : i64 -> tensor<1024x1xi64>
    %25 = arith.extsi %24 : tensor<1024x1xi32> to tensor<1024x1xi64>
    [parseAddPtr] offset is
%26 = arith.addi %9, %25 : tensor<1024x1xi64>
[parseAddPtr] ptrStructured:     1    1
    [parseAddPtr] offsetStructured:     0    1
finish parse: %27 = tt.addptr %10, %24 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32> structured: 0 1 
finish parse: %29 = tt.broadcast %27 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>> structured: 0 1 
parse: %12 = tt.broadcast %11 : tensor<1x16xi32> -> tensor<1024x16xi32>
parse: %11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi32> -> tensor<1x16xi32>
parse: %7 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
finish parse: %7 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32> structured: 1 
finish parse: %11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi32> -> tensor<1x16xi32> structured: 1 1 
finish parse: %12 = tt.broadcast %11 : tensor<1x16xi32> -> tensor<1024x16xi32> structured: 1 1 
    [parseAddPtr] Adding offset
    %28 = tt.broadcast %26 : tensor<1024x1xi64> -> tensor<1024x16xi64>
    %30 = arith.extsi %12 : tensor<1024x16xi32> to tensor<1024x16xi64>
    [parseAddPtr] offset is
%31 = arith.addi %28, %30 : tensor<1024x16xi64>
[parseAddPtr] ptrStructured:     0    1
    [parseAddPtr] offsetStructured:     1    1
finish parse: %32 = tt.addptr %29, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32> structured: 0 1 
KMLOG Parsing done for: %36 = tt.load %32, %35 : tensor<1024x16x!tt.ptr<f32>>
[  0x1]
tt.func public @origin_index_select(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c0_i64 = arith.constant 0 : i64
  %c0_i64_0 = arith.constant 0 : i64
  %c0_i64_1 = arith.constant 0 : i64
  %c0_i64_2 = arith.constant 0 : i64
  %c0_i64_3 = arith.constant 0 : i64
  %c1024_i32 = arith.constant 1024 : i32
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant dense<16> : tensor<1024x1xi32>
  %cst_4 = arith.constant dense<16> : tensor<16xi32>
  %cst_5 = arith.constant dense<0> : tensor<1024xi32>
  %cst_6 = arith.constant dense<1971940> : tensor<1024xi32>
  %c49299_i32 = arith.constant 49299 : i32
  %0 = tt.get_program_id x : i32
  %1 = arith.muli %0, %c49299_i32 : i32
  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
  %3 = tt.splat %1 : i32 -> tensor<1024xi32>
  %4 = arith.addi %2, %3 : tensor<1024xi32>
  %5 = tt.splat %c0_i64_3 : i64 -> tensor<1024xi64>
  %6 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>>
  %7 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
  %8 = arith.cmpi slt, %7, %cst_4 : tensor<16xi32>
  %9 = tt.splat %c0_i64_1 : i64 -> tensor<1024x1xi64>
  %10 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  %11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi32> -> tensor<1x16xi32>
  %12 = tt.broadcast %11 : tensor<1x16xi32> -> tensor<1024x16xi32>
  %13 = tt.expand_dims %8 {axis = 0 : i32} : tensor<16xi1> -> tensor<1x16xi1>
  %14 = tt.broadcast %13 : tensor<1x16xi1> -> tensor<1024x16xi1>
  %15 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  scf.for %arg4 = %c0_i32 to %c49299_i32 step %c1024_i32  : i32 {
    %c0_i64_7 = arith.constant 0 : i64
    %16 = tt.splat %arg4 : i32 -> tensor<1024xi32>
    %17 = arith.addi %4, %16 : tensor<1024xi32>
    %18 = arith.cmpi slt, %17, %cst_6 : tensor<1024xi32>
    %19 = arith.extsi %17 : tensor<1024xi32> to tensor<1024xi64>
    %20 = arith.addi %5, %19 : tensor<1024xi64>
    %21 = tt.addptr %6, %17 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
    %22 = tt.load %21, %18, %cst_5 : tensor<1024x!tt.ptr<i32>>
    %23 = tt.expand_dims %22 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %24 = arith.muli %23, %cst : tensor<1024x1xi32>
    %25 = arith.extsi %24 : tensor<1024x1xi32> to tensor<1024x1xi64>
    %26 = arith.addi %9, %25 : tensor<1024x1xi64>
    %27 = tt.addptr %10, %24 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %28 = tt.broadcast %26 : tensor<1024x1xi64> -> tensor<1024x16xi64>
    %29 = tt.broadcast %27 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %30 = arith.extsi %12 : tensor<1024x16xi32> to tensor<1024x16xi64>
    %31 = arith.addi %28, %30 : tensor<1024x16xi64>
    %32 = tt.addptr %29, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    %33 = tt.expand_dims %18 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
    %34 = tt.broadcast %33 : tensor<1024x1xi1> -> tensor<1024x16xi1>
    %35 = arith.andi %34, %14 : tensor<1024x16xi1>
    %36 = tt.load %32, %35 : tensor<1024x16x!tt.ptr<f32>>
    %37 = tt.expand_dims %17 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %38 = arith.muli %37, %cst : tensor<1024x1xi32>
    %39 = tt.addptr %15, %38 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %40 = tt.broadcast %39 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %41 = tt.addptr %40, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    tt.store %41, %36, %35 : tensor<1024x16x!tt.ptr<f32>>
  }
  tt.return
}
========================================================
KMLOG Parsing tt.store tt.store %41, %36, %35 : tensor<1024x16x!tt.ptr<f32>>parse: %41 = tt.addptr %40, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
parse: %40 = tt.broadcast %39 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
parse: %39 = tt.addptr %15, %38 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
parse: %15 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
parse: <block argument> of type '!tt.ptr<f32>' at index: 2
finish parse: <block argument> of type '!tt.ptr<f32>' at index: 2 structured: 
[parseSplat] dst is
%15 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
finish parse: %16 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>> structured: 1 1 
parse: %39 = arith.muli %38, %cst : tensor<1024x1xi32>
parse: %38 = tt.expand_dims %18 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
found: %18 = arith.addi %4, %17 : tensor<1024xi32>
finish parse: %38 = tt.expand_dims %18 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32> structured: 1 1 
found: %cst = arith.constant dense<16> : tensor<1024x1xi32>
finish parse: %39 = arith.muli %38, %cst : tensor<1024x1xi32> structured: 1 1 
    [parseAddPtr] Adding offset
    %15 = tt.splat %c0_i64_0 : i64 -> tensor<1024x1xi64>
    %40 = arith.extsi %39 : tensor<1024x1xi32> to tensor<1024x1xi64>
    [parseAddPtr] offset is
%41 = arith.addi %15, %40 : tensor<1024x1xi64>
[parseAddPtr] ptrStructured:     1    1
    [parseAddPtr] offsetStructured:     1    1
finish parse: %42 = tt.addptr %16, %39 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32> structured: 1 1 
finish parse: %44 = tt.broadcast %42 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>> structured: 1 1 
found: %12 = tt.broadcast %11 : tensor<1x16xi32> -> tensor<1024x16xi32>
    [parseAddPtr] Adding offset
    %43 = tt.broadcast %41 : tensor<1024x1xi64> -> tensor<1024x16xi64>
    %45 = arith.extsi %12 : tensor<1024x16xi32> to tensor<1024x16xi64>
    [parseAddPtr] offset is
%46 = arith.addi %43, %45 : tensor<1024x16xi64>
[parseAddPtr] ptrStructured:     1    1
    [parseAddPtr] offsetStructured:     1    1
finish parse: %47 = tt.addptr %44, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32> structured: 1 1 
KMLOG Parsing done for: tt.store %47, %37, %36 : tensor<1024x16x!tt.ptr<f32>>
KMLOG All parsing done
Parsing done
** Replace : 'arith.constant'(0x5e90836a6440)
** Erase   : 'arith.constant'(0x5e90836a6440)
** Replace : 'arith.constant'(0x5e90836a63d0)
** Modified: 'tt.splat'(0x5e90836a56b0)
** Erase   : 'arith.constant'(0x5e90836a63d0)
** Replace : 'arith.constant'(0x5e90836a5540)
** Erase   : 'arith.constant'(0x5e90836a5540)
** Replace : 'arith.constant'(0x5e90836a54d0)
** Erase   : 'arith.constant'(0x5e90836a54d0)
** Replace : 'arith.constant'(0x5e90836aef70)
** Modified: 'tt.splat'(0x5e90836ac3f0)
** Erase   : 'arith.constant'(0x5e90836aef70)
** Replace : 'arith.constant'(0x5e908367b1c0)
** Erase   : 'arith.constant'(0x5e908367b1c0)
** Replace : 'arith.constant'(0x5e90836a5440)
** Modified: 'tt.splat'(0x5e90836a69d0)
** Erase   : 'arith.constant'(0x5e90836a5440)
** Replace : 'arith.constant'(0x5e90836af510)
** Erase   : 'arith.constant'(0x5e90836af510)

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x5e9083650000) {
  "tt.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x5e9083646c30) {
  "scf.yield"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x5e9083664460) {
  "tt.store"(%55, %45, %44) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xf32>, tensor<1024x16xi1>) -> ()


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp> : 'tt.store -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp>"
KMLOG Before conversion
tt.func public @origin_index_select(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c0_i64 = arith.constant 0 : i64
  %c1024_i32 = arith.constant 1024 : i32
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant dense<16> : tensor<1024x1xi32>
  %cst_0 = arith.constant dense<16> : tensor<16xi32>
  %cst_1 = arith.constant dense<0> : tensor<1024xi32>
  %cst_2 = arith.constant dense<1971940> : tensor<1024xi32>
  %c49299_i32 = arith.constant 49299 : i32
  %0 = tt.get_program_id x : i32
  %1 = arith.muli %0, %c49299_i32 : i32
  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
  %3 = tt.splat %1 : i32 -> tensor<1024xi32>
  %4 = arith.addi %2, %3 : tensor<1024xi32>
  %5 = tt.splat %c0_i64 : i64 -> tensor<1024xi64>
  %6 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>>
  %7 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
  %8 = arith.cmpi slt, %7, %cst_0 : tensor<16xi32>
  %9 = tt.splat %c0_i64 : i64 -> tensor<1024x1xi64>
  %10 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  %11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi32> -> tensor<1x16xi32>
  %12 = tt.broadcast %11 : tensor<1x16xi32> -> tensor<1024x16xi32>
  %13 = tt.expand_dims %8 {axis = 0 : i32} : tensor<16xi1> -> tensor<1x16xi1>
  %14 = tt.broadcast %13 : tensor<1x16xi1> -> tensor<1024x16xi1>
  %15 = tt.splat %c0_i64 : i64 -> tensor<1024x1xi64>
  %16 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  scf.for %arg4 = %c0_i32 to %c49299_i32 step %c1024_i32  : i32 {
    %17 = tt.splat %arg4 : i32 -> tensor<1024xi32>
    %18 = arith.addi %4, %17 : tensor<1024xi32>
    %19 = arith.cmpi slt, %18, %cst_2 : tensor<1024xi32>
    %20 = arith.extsi %18 : tensor<1024xi32> to tensor<1024xi64>
    %21 = arith.addi %5, %20 : tensor<1024xi64>
    %22 = tt.addptr %6, %18 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
    %23 = tt.load %22, %19, %cst_1 : tensor<1024x!tt.ptr<i32>>
    %24 = tt.expand_dims %23 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %25 = arith.muli %24, %cst : tensor<1024x1xi32>
    %26 = arith.extsi %25 : tensor<1024x1xi32> to tensor<1024x1xi64>
    %27 = arith.addi %9, %26 : tensor<1024x1xi64>
    %28 = tt.addptr %10, %25 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %29 = tt.broadcast %27 : tensor<1024x1xi64> -> tensor<1024x16xi64>
    %30 = tt.broadcast %28 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %31 = arith.extsi %12 : tensor<1024x16xi32> to tensor<1024x16xi64>
    %32 = arith.addi %29, %31 : tensor<1024x16xi64>
    %33 = tt.addptr %30, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    %34 = tt.expand_dims %19 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
    %35 = tt.broadcast %34 : tensor<1024x1xi1> -> tensor<1024x16xi1>
    %36 = arith.andi %35, %14 : tensor<1024x16xi1>
    %37 = tt.load %33, %36 : tensor<1024x16x!tt.ptr<f32>>
    %38 = tt.expand_dims %18 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %39 = arith.muli %38, %cst : tensor<1024x1xi32>
    %40 = arith.extsi %39 : tensor<1024x1xi32> to tensor<1024x1xi64>
    %41 = arith.addi %15, %40 : tensor<1024x1xi64>
    %42 = tt.addptr %16, %39 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %43 = tt.broadcast %41 : tensor<1024x1xi64> -> tensor<1024x16xi64>
    %44 = tt.broadcast %42 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %45 = arith.extsi %12 : tensor<1024x16xi32> to tensor<1024x16xi64>
    %46 = arith.addi %43, %45 : tensor<1024x16xi64>
    %47 = tt.addptr %44, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    tt.store %47, %37, %36 : tensor<1024x16x!tt.ptr<f32>>
  }
  tt.return
}
KMLOG ptrOffsetInfo dump
11
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cf40) {
  %55 = "tt.addptr"(%52, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836adab0) {
  %54 = "arith.addi"(%51, %53) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>

  ** Erase   : 'arith.addi'(0x5e90836adab0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836afad0) {
  %53 = "arith.extsi"(%20) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>

  ** Erase   : 'arith.extsi'(0x5e90836afad0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366ce50) {
  %52 = "tt.broadcast"(%50) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836af9d0) {
  %51 = "tt.broadcast"(%49) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>

  ** Erase   : 'tt.broadcast'(0x5e90836af9d0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cd40) {
  %50 = "tt.addptr"(%24, %47) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836a65c0) {
  %49 = "arith.addi"(%23, %48) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>

  ** Erase   : 'arith.addi'(0x5e90836a65c0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836a64e0) {
  %48 = "arith.extsi"(%47) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>

  ** Erase   : 'arith.extsi'(0x5e90836a64e0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366cc30) {
  %47 = "arith.muli"(%46, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366cb40) {
  %46 = "tt.expand_dims"(%26) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e908365f8d0) {
  %45 = "tt.load"(%41, %44) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 0>}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi1>) -> tensor<1024x16xf32>


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp> : 'tt.load -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>"
KMLOG Before conversion
tt.func public @origin_index_select(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<i32> {tt.divisibility = 16 : i32}, %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg3: i32 {tt.divisibility = 16 : i32}) attributes {noinline = false} {
  %c0_i64 = arith.constant 0 : i64
  %c1024_i32 = arith.constant 1024 : i32
  %c0_i32 = arith.constant 0 : i32
  %cst = arith.constant dense<16> : tensor<1024x1xi32>
  %cst_0 = arith.constant dense<16> : tensor<16xi32>
  %cst_1 = arith.constant dense<0> : tensor<1024xi32>
  %cst_2 = arith.constant dense<1971940> : tensor<1024xi32>
  %c49299_i32 = arith.constant 49299 : i32
  %0 = tt.get_program_id x : i32
  %1 = arith.muli %0, %c49299_i32 : i32
  %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>
  %3 = tt.splat %1 : i32 -> tensor<1024xi32>
  %4 = arith.addi %2, %3 : tensor<1024xi32>
  %5 = tt.splat %c0_i64 : i64 -> tensor<1024xi64>
  %6 = tt.splat %arg1 : !tt.ptr<i32> -> tensor<1024x!tt.ptr<i32>>
  %7 = tt.make_range {end = 16 : i32, start = 0 : i32} : tensor<16xi32>
  %8 = arith.cmpi slt, %7, %cst_0 : tensor<16xi32>
  %9 = tt.splat %c0_i64 : i64 -> tensor<1024x1xi64>
  %10 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  %11 = tt.expand_dims %7 {axis = 0 : i32} : tensor<16xi32> -> tensor<1x16xi32>
  %12 = tt.broadcast %11 : tensor<1x16xi32> -> tensor<1024x16xi32>
  %13 = tt.expand_dims %8 {axis = 0 : i32} : tensor<16xi1> -> tensor<1x16xi1>
  %14 = tt.broadcast %13 : tensor<1x16xi1> -> tensor<1024x16xi1>
  %15 = tt.splat %c0_i64 : i64 -> tensor<1024x1xi64>
  %16 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x1x!tt.ptr<f32>>
  scf.for %arg4 = %c0_i32 to %c49299_i32 step %c1024_i32  : i32 {
    %17 = tt.splat %arg4 : i32 -> tensor<1024xi32>
    %18 = arith.addi %4, %17 : tensor<1024xi32>
    %19 = arith.cmpi slt, %18, %cst_2 : tensor<1024xi32>
    %20 = arith.extsi %18 : tensor<1024xi32> to tensor<1024xi64>
    %21 = arith.addi %5, %20 : tensor<1024xi64>
    %22 = tt.addptr %6, %18 : tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>
    %23 = tt.load %22, %19, %cst_1 : tensor<1024x!tt.ptr<i32>>
    %24 = tt.expand_dims %23 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %25 = arith.muli %24, %cst : tensor<1024x1xi32>
    %26 = arith.extsi %25 : tensor<1024x1xi32> to tensor<1024x1xi64>
    %27 = arith.addi %9, %26 : tensor<1024x1xi64>
    %28 = tt.addptr %10, %25 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %29 = tt.broadcast %27 : tensor<1024x1xi64> -> tensor<1024x16xi64>
    %30 = tt.broadcast %28 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %31 = arith.extsi %12 : tensor<1024x16xi32> to tensor<1024x16xi64>
    %32 = arith.addi %29, %31 : tensor<1024x16xi64>
    %33 = tt.addptr %30, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    %34 = tt.expand_dims %19 {axis = 1 : i32} : tensor<1024xi1> -> tensor<1024x1xi1>
    %35 = tt.broadcast %34 : tensor<1024x1xi1> -> tensor<1024x16xi1>
    %36 = arith.andi %35, %14 : tensor<1024x16xi1>
    %37 = tt.load %33, %36 : tensor<1024x16x!tt.ptr<f32>>
    %38 = tt.expand_dims %18 {axis = 1 : i32} : tensor<1024xi32> -> tensor<1024x1xi32>
    %39 = arith.muli %38, %cst : tensor<1024x1xi32>
    %40 = tt.addptr %16, %39 : tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>
    %41 = tt.broadcast %40 : tensor<1024x1x!tt.ptr<f32>> -> tensor<1024x16x!tt.ptr<f32>>
    %42 = tt.addptr %41, %12 : tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>
    tt.store %42, %37, %36 : tensor<1024x16x!tt.ptr<f32>>
  }
  tt.return
}
KMLOG ptrOffsetInfo dump
01
Converting tt.load
%37 = tt.load %33, %36 : tensor<1024x16x!tt.ptr<f32>>
0
    ** Insert  : 'arith.constant'(0x5e90836a5440)
    ** Insert  : 'arith.constant'(0x5e908367b1c0)
resultShape 1
ImplicitTypeIDRegistry::lookupOrInsert(mlir::ReifyRankedShapedTypeOpInterface::Trait<Empty>)
    ** Insert  : 'tensor.empty'(0x5e90836aef70)
    ** Insert  : 'arith.constant'(0x5e90836a54d0)
    ** Insert  : 'scf.for'(0x5e90836af8d0)
    ** Insert  : 'tensor.extract'(0x5e90836a65c0)
    ** Insert  : 'tt.addptr'(0x5e90836adab0)
    ** Insert  : 'tt.load'(0x5e90836aa490)
    ** Insert  : 'tt.splat'(0x5e90836a64e0)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::tensor::detail::InsertSliceOpGenericAdaptorBase::Properties)
    ** Insert  : 'tensor.insert_slice'(0x5e90836ae9a0)
    ** Insert  : 'scf.yield'(0x5e90836aceb0)
    ** Replace : 'tt.load'(0x5e908365f8d0)
    ** Modified: 'tt.store'(0x5e9083664460)
    ** Erase   : 'tt.load'(0x5e908365f8d0)
KMLOG After conversion
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %1 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %3 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %4 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %5 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %6 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %7 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %8 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %9 = "arith.muli"(%8, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %10 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %11 = "tt.splat"(%9) : (i32) -> tensor<1024xi32>
  %12 = "arith.addi"(%10, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %13 = "tt.splat"(%0) : (i64) -> tensor<1024xi64>
  %14 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %15 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %16 = "arith.cmpi"(%15, %4) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %17 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %18 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  %19 = "tt.expand_dims"(%15) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %20 = "tt.broadcast"(%19) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %21 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %22 = "tt.broadcast"(%21) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %23 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %24 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%2, %7, %1) ({
  ^bb0(%arg4: i32):
    %25 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %26 = "arith.addi"(%12, %25) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %27 = "arith.cmpi"(%26, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %28 = "arith.extsi"(%26) : (tensor<1024xi32>) -> tensor<1024xi64>
    %29 = "arith.addi"(%13, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi64>, tensor<1024xi64>) -> tensor<1024xi64>
    %30 = "tt.addptr"(%14, %26) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %31 = "tt.load"(%30, %27, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %32 = "tt.expand_dims"(%31) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %33 = "arith.muli"(%32, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %34 = "arith.extsi"(%33) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %35 = "arith.addi"(%17, %34) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>
    %36 = "tt.addptr"(%18, %33) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %37 = "tt.broadcast"(%35) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %38 = "tt.broadcast"(%36) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %39 = "arith.extsi"(%20) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %41 = "tt.addptr"(%38, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    %42 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %43 = "tt.broadcast"(%42) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %44 = "arith.andi"(%43, %22) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %45 = "arith.constant"() <{value = 0 : index}> : () -> index
    %46 = "arith.constant"() <{value = 1 : index}> : () -> index
    %47 = "tensor.empty"() : () -> tensor<1024xf32>
    %48 = "arith.constant"() <{value = 1024 : index}> : () -> index
    %49 = "scf.for"(%45, %48, %46, %47) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %55 = "tensor.extract"(%40, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %56 = "tt.addptr"(%arg0, %55) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %57 = "tt.load"(%56) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %58 = "tt.splat"(%57) : (f32) -> tensor<16xf32>
      %59 = "tensor.insert_slice"(%58, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%59) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %50 = "tt.expand_dims"(%26) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %51 = "arith.muli"(%50, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %52 = "tt.addptr"(%24, %51) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %53 = "tt.broadcast"(%52) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %54 = "tt.addptr"(%53, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%54, %49, %44) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>" result 1
  } -> success : pattern applied successfully
// *** IR Dump After Pattern Application ***
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %1 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %3 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %4 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %5 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %6 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %7 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %8 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %9 = "arith.muli"(%8, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %10 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %11 = "tt.splat"(%9) : (i32) -> tensor<1024xi32>
  %12 = "arith.addi"(%10, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %13 = "tt.splat"(%0) : (i64) -> tensor<1024xi64>
  %14 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %15 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %16 = "arith.cmpi"(%15, %4) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %17 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %18 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  %19 = "tt.expand_dims"(%15) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %20 = "tt.broadcast"(%19) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %21 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %22 = "tt.broadcast"(%21) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %23 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %24 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%2, %7, %1) ({
  ^bb0(%arg4: i32):
    %25 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %26 = "arith.addi"(%12, %25) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %27 = "arith.cmpi"(%26, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %28 = "arith.extsi"(%26) : (tensor<1024xi32>) -> tensor<1024xi64>
    %29 = "arith.addi"(%13, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi64>, tensor<1024xi64>) -> tensor<1024xi64>
    %30 = "tt.addptr"(%14, %26) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %31 = "tt.load"(%30, %27, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %32 = "tt.expand_dims"(%31) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %33 = "arith.muli"(%32, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %34 = "arith.extsi"(%33) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %35 = "arith.addi"(%17, %34) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>
    %36 = "tt.addptr"(%18, %33) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %37 = "tt.broadcast"(%35) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %38 = "tt.broadcast"(%36) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %39 = "arith.extsi"(%20) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %41 = "tt.addptr"(%38, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    %42 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %43 = "tt.broadcast"(%42) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %44 = "arith.andi"(%43, %22) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %45 = "arith.constant"() <{value = 0 : index}> : () -> index
    %46 = "arith.constant"() <{value = 1 : index}> : () -> index
    %47 = "tensor.empty"() : () -> tensor<1024xf32>
    %48 = "arith.constant"() <{value = 1024 : index}> : () -> index
    %49 = "scf.for"(%45, %48, %46, %47) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %55 = "tensor.extract"(%40, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %56 = "tt.addptr"(%arg0, %55) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %57 = "tt.load"(%56) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %58 = "tt.splat"(%57) : (f32) -> tensor<16xf32>
      %59 = "tensor.insert_slice"(%58, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%59) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %50 = "tt.expand_dims"(%26) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %51 = "arith.muli"(%50, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %52 = "tt.addptr"(%24, %51) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %53 = "tt.broadcast"(%52) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %54 = "tt.addptr"(%53, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%54, %49, %44) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()


} -> success : pattern matched
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x5e9083664460) {
  "tt.store"(%54, %49, %44) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp> : 'tt.store -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp>"
KMLOG Before conversion
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %1 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %3 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %4 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %5 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %6 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %7 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %8 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %9 = "arith.muli"(%8, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %10 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %11 = "tt.splat"(%9) : (i32) -> tensor<1024xi32>
  %12 = "arith.addi"(%10, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %13 = "tt.splat"(%0) : (i64) -> tensor<1024xi64>
  %14 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %15 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %16 = "arith.cmpi"(%15, %4) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %17 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %18 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  %19 = "tt.expand_dims"(%15) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %20 = "tt.broadcast"(%19) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %21 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %22 = "tt.broadcast"(%21) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %23 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %24 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%2, %7, %1) ({
  ^bb0(%arg4: i32):
    %25 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %26 = "arith.addi"(%12, %25) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %27 = "arith.cmpi"(%26, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %28 = "arith.extsi"(%26) : (tensor<1024xi32>) -> tensor<1024xi64>
    %29 = "arith.addi"(%13, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi64>, tensor<1024xi64>) -> tensor<1024xi64>
    %30 = "tt.addptr"(%14, %26) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %31 = "tt.load"(%30, %27, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %32 = "tt.expand_dims"(%31) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %33 = "arith.muli"(%32, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %34 = "arith.extsi"(%33) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %35 = "arith.addi"(%17, %34) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>
    %36 = "tt.addptr"(%18, %33) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %37 = "tt.broadcast"(%35) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %38 = "tt.broadcast"(%36) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %39 = "arith.extsi"(%20) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %41 = "tt.addptr"(%38, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    %42 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %43 = "tt.broadcast"(%42) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %44 = "arith.andi"(%43, %22) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %45 = "arith.constant"() <{value = 0 : index}> : () -> index
    %46 = "arith.constant"() <{value = 1 : index}> : () -> index
    %47 = "tensor.empty"() : () -> tensor<1024xf32>
    %48 = "arith.constant"() <{value = 1024 : index}> : () -> index
    %49 = "scf.for"(%45, %48, %46, %47) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %55 = "tensor.extract"(%40, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %56 = "tt.addptr"(%arg0, %55) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %57 = "tt.load"(%56) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %58 = "tt.splat"(%57) : (f32) -> tensor<16xf32>
      %59 = "tensor.insert_slice"(%58, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%59) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %50 = "tt.expand_dims"(%26) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %51 = "arith.muli"(%50, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %52 = "tt.addptr"(%24, %51) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %53 = "tt.broadcast"(%52) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %54 = "tt.addptr"(%53, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%54, %49, %44) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()
KMLOG ptrOffsetInfo dump
11
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x5e90836aceb0) {
  "scf.yield"(%59) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()

ImplicitTypeIDRegistry::lookupOrInsert(mlir::DestinationStyleOpInterface::Trait<Empty>)
ImplicitTypeIDRegistry::lookupOrInsert(mlir::OffsetSizeAndStrideOpInterface::Trait<Empty>)
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x5e90836ae9a0) {
  %59 = "tensor.insert_slice"(%58, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e90836a64e0) {
  %58 = "tt.splat"(%57) : (f32) -> tensor<16xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e90836aa490) {
  %57 = "tt.load"(%56) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp> : 'tt.load -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>"
KMLOG Before conversion
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %1 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %3 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %4 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %5 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %6 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %7 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %8 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %9 = "arith.muli"(%8, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %10 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %11 = "tt.splat"(%9) : (i32) -> tensor<1024xi32>
  %12 = "arith.addi"(%10, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %13 = "tt.splat"(%0) : (i64) -> tensor<1024xi64>
  %14 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %15 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %16 = "arith.cmpi"(%15, %4) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %17 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %18 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  %19 = "tt.expand_dims"(%15) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %20 = "tt.broadcast"(%19) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %21 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %22 = "tt.broadcast"(%21) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %23 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %24 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%2, %7, %1) ({
  ^bb0(%arg4: i32):
    %25 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %26 = "arith.addi"(%12, %25) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %27 = "arith.cmpi"(%26, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %28 = "arith.extsi"(%26) : (tensor<1024xi32>) -> tensor<1024xi64>
    %29 = "arith.addi"(%13, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi64>, tensor<1024xi64>) -> tensor<1024xi64>
    %30 = "tt.addptr"(%14, %26) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %31 = "tt.load"(%30, %27, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %32 = "tt.expand_dims"(%31) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %33 = "arith.muli"(%32, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %34 = "arith.extsi"(%33) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %35 = "arith.addi"(%17, %34) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>
    %36 = "tt.addptr"(%18, %33) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %37 = "tt.broadcast"(%35) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %38 = "tt.broadcast"(%36) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %39 = "arith.extsi"(%20) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %41 = "tt.addptr"(%38, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    %42 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %43 = "tt.broadcast"(%42) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %44 = "arith.andi"(%43, %22) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %45 = "arith.constant"() <{value = 0 : index}> : () -> index
    %46 = "arith.constant"() <{value = 1 : index}> : () -> index
    %47 = "tensor.empty"() : () -> tensor<1024xf32>
    %48 = "arith.constant"() <{value = 1024 : index}> : () -> index
    %49 = "scf.for"(%45, %48, %46, %47) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %55 = "tensor.extract"(%40, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %56 = "tt.addptr"(%arg0, %55) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %57 = "tt.load"(%56) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %58 = "tt.splat"(%57) : (f32) -> tensor<16xf32>
      %59 = "tensor.insert_slice"(%58, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%59) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %50 = "tt.expand_dims"(%26) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %51 = "arith.muli"(%50, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %52 = "tt.addptr"(%24, %51) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %53 = "tt.broadcast"(%52) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %54 = "tt.addptr"(%53, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%54, %49, %44) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e90836adab0) {
  %56 = "tt.addptr"(%arg0, %55) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.extract'(0x5e90836a65c0) {
  %55 = "tensor.extract"(%40, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e90836af8d0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836a54d0) {
  %48 = "arith.constant"() <{value = 1024 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5e90836aef70) {
  %47 = "tensor.empty"() : () -> tensor<1024xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e908367b1c0) {
  %46 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836a5440) {
  %45 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x5e9083669fb0) {
  %44 = "arith.andi"(%43, %22) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366c200) {
  %43 = "tt.broadcast"(%42) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366c110) {
  %42 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366c000) {
  %41 = "tt.addptr"(%38, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

  ** Erase   : 'tt.addptr'(0x5e908366c000)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836accc0) {
  %40 = "arith.addi"(%37, %39) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836ad560) {
  %39 = "arith.extsi"(%20) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366bf10) {
  %38 = "tt.broadcast"(%36) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

  ** Erase   : 'tt.broadcast'(0x5e908366bf10)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836a5bb0) {
  %37 = "tt.broadcast"(%35) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366be00) {
  %36 = "tt.addptr"(%18, %33) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

  ** Erase   : 'tt.addptr'(0x5e908366be00)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836ada00) {
  %35 = "arith.addi"(%17, %34) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836acd70) {
  %34 = "arith.extsi"(%33) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366bcf0) {
  %33 = "arith.muli"(%32, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366b780) {
  %32 = "tt.expand_dims"(%31) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e90836419a0) {
  %31 = "tt.load"(%30, %27, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp> : 'tt.load -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>"
KMLOG Before conversion
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %1 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %3 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %4 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %5 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %6 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %7 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %8 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %9 = "arith.muli"(%8, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %10 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %11 = "tt.splat"(%9) : (i32) -> tensor<1024xi32>
  %12 = "arith.addi"(%10, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %13 = "tt.splat"(%0) : (i64) -> tensor<1024xi64>
  %14 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %15 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %16 = "arith.cmpi"(%15, %4) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %17 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %18 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  %19 = "tt.expand_dims"(%15) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %20 = "tt.broadcast"(%19) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %21 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %22 = "tt.broadcast"(%21) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %23 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %24 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%2, %7, %1) ({
  ^bb0(%arg4: i32):
    %25 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %26 = "arith.addi"(%12, %25) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %27 = "arith.cmpi"(%26, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %28 = "arith.extsi"(%26) : (tensor<1024xi32>) -> tensor<1024xi64>
    %29 = "arith.addi"(%13, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi64>, tensor<1024xi64>) -> tensor<1024xi64>
    %30 = "tt.addptr"(%14, %26) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %31 = "tt.load"(%30, %27, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %32 = "tt.expand_dims"(%31) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %33 = "arith.muli"(%32, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %34 = "arith.extsi"(%33) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %35 = "arith.addi"(%17, %34) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>
    %36 = "tt.broadcast"(%35) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %37 = "arith.extsi"(%20) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %38 = "arith.addi"(%36, %37) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %39 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %40 = "tt.broadcast"(%39) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %41 = "arith.andi"(%40, %22) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %42 = "arith.constant"() <{value = 0 : index}> : () -> index
    %43 = "arith.constant"() <{value = 1 : index}> : () -> index
    %44 = "tensor.empty"() : () -> tensor<1024xf32>
    %45 = "arith.constant"() <{value = 1024 : index}> : () -> index
    %46 = "scf.for"(%42, %45, %43, %44) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %52 = "tensor.extract"(%38, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %53 = "tt.addptr"(%arg0, %52) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %54 = "tt.load"(%53) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %55 = "tt.splat"(%54) : (f32) -> tensor<16xf32>
      %56 = "tensor.insert_slice"(%55, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%56) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %47 = "tt.expand_dims"(%26) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %48 = "arith.muli"(%47, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %49 = "tt.addptr"(%24, %48) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %50 = "tt.broadcast"(%49) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %51 = "tt.addptr"(%50, %20) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%51, %46, %41) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()
KMLOG ptrOffsetInfo dump
1
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366ad10) {
  %30 = "tt.addptr"(%14, %26) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e908365f5e0) {
  %29 = "arith.addi"(%13, %28) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi64>, tensor<1024xi64>) -> tensor<1024xi64>

  ** Erase   : 'arith.addi'(0x5e908365f5e0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836acc30) {
  %28 = "arith.extsi"(%26) : (tensor<1024xi32>) -> tensor<1024xi64>

  ** Erase   : 'arith.extsi'(0x5e90836acc30)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e908366ac20) {
  %27 = "arith.cmpi"(%26, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e908366a670) {
  %26 = "arith.addi"(%12, %25) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e908366d040) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e908366a100) {
  %25 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083669330) {
  %24 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e9083668dc0) {
  %22 = "tt.broadcast"(%21) : (tensor<1x16xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083668cd0) {
  %21 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836687a0) {
  %20 = "tt.broadcast"(%19) : (tensor<1x16xi32>) -> tensor<1024x16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083649170) {
  %19 = "tt.expand_dims"(%15) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083648c00) {
  %18 = "tt.splat"(%arg0) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

  ** Erase   : 'tt.splat'(0x5e9083648c00)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e9083648b10) {
  %16 = "arith.cmpi"(%15, %4) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e9083666b90) {
  %15 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083666aa0) {
  %14 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e9083666510) {
  %12 = "arith.addi"(%10, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083665b20) {
  %11 = "tt.splat"(%9) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e90836655d0) {
  %10 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e90836646f0) {
  %9 = "arith.muli"(%8, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5e9083653f20) {
  %8 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836645e0) {
  %7 = "arith.constant"() <{value = 49299 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662c20) {
  %6 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083664400) {
  %5 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836635b0) {
  %4 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662730) {
  %3 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836613f0) {
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083660580) {
  %1 = "arith.constant"() <{value = 1024 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e90836a69d0) {
  %13 = "tt.splat"(%0) : (i64) -> tensor<1024xi64>

  ** Erase   : 'tt.splat'(0x5e90836a69d0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e90836ac3f0) {
  %16 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>

} -> success : operation was folded
//===-------------------------------------------===//
** Insert  : 'arith.constant'(0x5e90836a63d0)
** Replace : 'tt.splat'(0x5e90836ac3f0)
** Modified: 'arith.addi'(0x5e90836ada00)
** Erase   : 'tt.splat'(0x5e90836ac3f0)
// *** IR Dump After Successful Folding ***
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 0 : i64}> : () -> i64
  %1 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %2 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %3 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %4 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %5 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %6 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %7 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %8 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %9 = "arith.muli"(%8, %7) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %10 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %11 = "tt.splat"(%9) : (i32) -> tensor<1024xi32>
  %12 = "arith.addi"(%10, %11) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %13 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %14 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %15 = "arith.cmpi"(%14, %4) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %16 = "arith.constant"() <{value = dense<0> : tensor<1024x1xi64>}> : () -> tensor<1024x1xi64>
  %17 = "tt.expand_dims"(%14) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %18 = "tt.broadcast"(%17) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %19 = "tt.expand_dims"(%15) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %20 = "tt.broadcast"(%19) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %21 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>
  %22 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%2, %7, %1) ({
  ^bb0(%arg4: i32):
    %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %24 = "arith.addi"(%12, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %25 = "arith.cmpi"(%24, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %26 = "tt.addptr"(%13, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %27 = "tt.load"(%26, %25, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %29 = "arith.muli"(%28, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %31 = "arith.addi"(%16, %30) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>
    %32 = "tt.broadcast"(%31) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %33 = "arith.extsi"(%18) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %34 = "arith.addi"(%32, %33) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %35 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %36 = "tt.broadcast"(%35) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %37 = "arith.andi"(%36, %20) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %38 = "arith.constant"() <{value = 0 : index}> : () -> index
    %39 = "arith.constant"() <{value = 1 : index}> : () -> index
    %40 = "tensor.empty"() : () -> tensor<1024xf32>
    %41 = "arith.constant"() <{value = 1024 : index}> : () -> index
    %42 = "scf.for"(%38, %41, %39, %40) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %48 = "tensor.extract"(%34, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %49 = "tt.addptr"(%arg0, %48) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %50 = "tt.load"(%49) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %51 = "tt.splat"(%50) : (f32) -> tensor<16xf32>
      %52 = "tensor.insert_slice"(%51, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%52) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %43 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %44 = "arith.muli"(%43, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %45 = "tt.addptr"(%22, %44) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %46 = "tt.broadcast"(%45) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %47 = "tt.addptr"(%46, %18) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%47, %42, %37) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()



//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e908366d040) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836ada00) {
  %31 = "arith.addi"(%16, %30) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>

} -> success : operation was folded
//===-------------------------------------------===//
** Modified: 'arith.addi'(0x5e90836ada00)
// *** IR Dump After Successful Folding ***
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"scf.for"(%2, %7, %1) ({
^bb0(%arg4: i32):
  %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
  %24 = "arith.addi"(%12, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %25 = "arith.cmpi"(%24, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
  %26 = "tt.addptr"(%13, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
  %27 = "tt.load"(%26, %25, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
  %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
  %29 = "arith.muli"(%28, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
  %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
  %31 = "arith.addi"(%30, %16) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>
  %32 = "tt.broadcast"(%31) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
  %33 = "arith.extsi"(%18) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
  %34 = "arith.addi"(%32, %33) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
  %35 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
  %36 = "tt.broadcast"(%35) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
  %37 = "arith.andi"(%36, %20) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
  %38 = "arith.constant"() <{value = 0 : index}> : () -> index
  %39 = "arith.constant"() <{value = 1 : index}> : () -> index
  %40 = "tensor.empty"() : () -> tensor<1024xf32>
  %41 = "arith.constant"() <{value = 1024 : index}> : () -> index
  %42 = "scf.for"(%38, %41, %39, %40) ({
  ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
    %48 = "tensor.extract"(%34, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
    %49 = "tt.addptr"(%arg0, %48) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
    %50 = "tt.load"(%49) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
    %51 = "tt.splat"(%50) : (f32) -> tensor<16xf32>
    %52 = "tensor.insert_slice"(%51, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
    "scf.yield"(%52) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
  }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
  %43 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
  %44 = "arith.muli"(%43, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
  %45 = "tt.addptr"(%22, %44) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
  %46 = "tt.broadcast"(%45) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
  %47 = "tt.addptr"(%46, %18) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
  "tt.store"(%47, %42, %37) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
  "scf.yield"() : () -> ()
}) : (i32, i32, i32) -> ()



//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e908366d040) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836ada00) {
  %31 = "arith.addi"(%30, %16) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi64>, tensor<1024x1xi64>) -> tensor<1024x1xi64>

} -> success : operation was folded
//===-------------------------------------------===//
** Replace : 'arith.addi'(0x5e90836ada00)
** Modified: 'tt.broadcast'(0x5e90836a5bb0)
** Erase   : 'arith.addi'(0x5e90836ada00)
// *** IR Dump After Successful Folding ***
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"scf.for"(%2, %7, %1) ({
^bb0(%arg4: i32):
  %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
  %24 = "arith.addi"(%12, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %25 = "arith.cmpi"(%24, %6) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
  %26 = "tt.addptr"(%13, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
  %27 = "tt.load"(%26, %25, %5) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
  %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
  %29 = "arith.muli"(%28, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
  %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
  %31 = "tt.broadcast"(%30) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
  %32 = "arith.extsi"(%18) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
  %33 = "arith.addi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
  %34 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
  %35 = "tt.broadcast"(%34) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
  %36 = "arith.andi"(%35, %20) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
  %37 = "arith.constant"() <{value = 0 : index}> : () -> index
  %38 = "arith.constant"() <{value = 1 : index}> : () -> index
  %39 = "tensor.empty"() : () -> tensor<1024xf32>
  %40 = "arith.constant"() <{value = 1024 : index}> : () -> index
  %41 = "scf.for"(%37, %40, %38, %39) ({
  ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
    %47 = "tensor.extract"(%33, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
    %48 = "tt.addptr"(%arg0, %47) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
    %49 = "tt.load"(%48) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
    %50 = "tt.splat"(%49) : (f32) -> tensor<16xf32>
    %51 = "tensor.insert_slice"(%50, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
    "scf.yield"(%51) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
  }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
  %42 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
  %43 = "arith.muli"(%42, %3) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
  %44 = "tt.addptr"(%22, %43) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
  %45 = "tt.broadcast"(%44) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
  %46 = "tt.addptr"(%45, %18) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
  "tt.store"(%46, %41, %36) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
  "scf.yield"() : () -> ()
}) : (i32, i32, i32) -> ()



//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836acd70) {
  %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e908366d040) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836a5bb0) {
  %31 = "tt.broadcast"(%30) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836a63d0) {
  %16 = "arith.constant"() <{value = dense<0> : tensor<1024x1xi64>}> : () -> tensor<1024x1xi64>

  ** Erase   : 'arith.constant'(0x5e90836a63d0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e90836a56b0) {
  %20 = "tt.splat"(%0) : (i64) -> tensor<1024x1xi64>

  ** Erase   : 'tt.splat'(0x5e90836a56b0)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x5e908366d1b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836afa60) {
  %0 = "arith.constant"() <{value = 0 : i64}> : () -> i64

  ** Erase   : 'arith.constant'(0x5e90836afa60)
} -> success : operation is trivially dead
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x5e9083650000) {
  "tt.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x5e9083646c30) {
  "scf.yield"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x5e9083664460) {
  "tt.store"(%43, %38, %36) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp> : 'tt.store -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp>"
KMLOG Before conversion
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 1024 : index}> : () -> index
  %1 = "arith.constant"() <{value = 1 : index}> : () -> index
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %5 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %6 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %7 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %8 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %9 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %10 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %11 = "arith.muli"(%10, %9) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %12 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %13 = "tt.splat"(%11) : (i32) -> tensor<1024xi32>
  %14 = "arith.addi"(%12, %13) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %15 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %16 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %17 = "arith.cmpi"(%16, %6) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %18 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %20 = "tt.expand_dims"(%17) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %21 = "tt.broadcast"(%20) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %22 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%4, %9, %3) ({
  ^bb0(%arg4: i32):
    %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %24 = "arith.addi"(%14, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %25 = "arith.cmpi"(%24, %8) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %26 = "tt.addptr"(%15, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %27 = "tt.load"(%26, %25, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %29 = "arith.muli"(%28, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %31 = "tt.broadcast"(%30) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %32 = "arith.extsi"(%19) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %33 = "arith.addi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %34 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %35 = "tt.broadcast"(%34) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %36 = "arith.andi"(%35, %21) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %37 = "tensor.empty"() : () -> tensor<1024xf32>
    %38 = "scf.for"(%2, %0, %1, %37) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %44 = "tensor.extract"(%33, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %45 = "tt.addptr"(%arg0, %44) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %46 = "tt.load"(%45) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %47 = "tt.splat"(%46) : (f32) -> tensor<16xf32>
      %48 = "tensor.insert_slice"(%47, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%48) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %39 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %40 = "arith.muli"(%39, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %41 = "tt.addptr"(%22, %40) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %42 = "tt.broadcast"(%41) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %43 = "tt.addptr"(%42, %19) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%43, %38, %36) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()
KMLOG ptrOffsetInfo dump
11
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::StoreOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cf40) {
  %43 = "tt.addptr"(%42, %19) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366ce50) {
  %42 = "tt.broadcast"(%41) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cd40) {
  %41 = "tt.addptr"(%22, %40) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366cc30) {
  %40 = "arith.muli"(%39, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366cb40) {
  %39 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x5e90836aceb0) {
  "scf.yield"(%48) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x5e90836ae9a0) {
  %48 = "tensor.insert_slice"(%47, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e90836a64e0) {
  %47 = "tt.splat"(%46) : (f32) -> tensor<16xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e90836aa490) {
  %46 = "tt.load"(%45) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp> : 'tt.load -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>"
KMLOG Before conversion
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 1024 : index}> : () -> index
  %1 = "arith.constant"() <{value = 1 : index}> : () -> index
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %5 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %6 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %7 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %8 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %9 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %10 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %11 = "arith.muli"(%10, %9) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %12 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %13 = "tt.splat"(%11) : (i32) -> tensor<1024xi32>
  %14 = "arith.addi"(%12, %13) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %15 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %16 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %17 = "arith.cmpi"(%16, %6) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %18 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %20 = "tt.expand_dims"(%17) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %21 = "tt.broadcast"(%20) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %22 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%4, %9, %3) ({
  ^bb0(%arg4: i32):
    %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %24 = "arith.addi"(%14, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %25 = "arith.cmpi"(%24, %8) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %26 = "tt.addptr"(%15, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %27 = "tt.load"(%26, %25, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %29 = "arith.muli"(%28, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %31 = "tt.broadcast"(%30) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %32 = "arith.extsi"(%19) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %33 = "arith.addi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %34 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %35 = "tt.broadcast"(%34) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %36 = "arith.andi"(%35, %21) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %37 = "tensor.empty"() : () -> tensor<1024xf32>
    %38 = "scf.for"(%2, %0, %1, %37) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %44 = "tensor.extract"(%33, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %45 = "tt.addptr"(%arg0, %44) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %46 = "tt.load"(%45) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %47 = "tt.splat"(%46) : (f32) -> tensor<16xf32>
      %48 = "tensor.insert_slice"(%47, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%48) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %39 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %40 = "arith.muli"(%39, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %41 = "tt.addptr"(%22, %40) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %42 = "tt.broadcast"(%41) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %43 = "tt.addptr"(%42, %19) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%43, %38, %36) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e90836adab0) {
  %45 = "tt.addptr"(%arg0, %44) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e90836af8d0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.extract'(0x5e90836a65c0) {
  %44 = "tensor.extract"(%33, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836a54d0) {
  %0 = "arith.constant"() <{value = 1024 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5e90836aef70) {
  %37 = "tensor.empty"() : () -> tensor<1024xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e908367b1c0) {
  %1 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836a5440) {
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x5e9083669fb0) {
  %36 = "arith.andi"(%35, %21) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366c200) {
  %35 = "tt.broadcast"(%34) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366c110) {
  %34 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836accc0) {
  %33 = "arith.addi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836ad560) {
  %32 = "arith.extsi"(%19) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836a5bb0) {
  %31 = "tt.broadcast"(%30) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836acd70) {
  %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366bcf0) {
  %29 = "arith.muli"(%28, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366b780) {
  %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e90836419a0) {
  %27 = "tt.load"(%26, %25, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>


  * Pattern (anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp> : 'tt.load -> ()' {
Trying to match "(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>"
KMLOG Before conversion
'tensor.extract' op incorrect number of indices for extract_element
mlir-asm-printer: 'tt.func' failed to verify and will be printed in generic form
"tt.func"() <{arg_attrs = [{tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}, {tt.divisibility = 16 : i32}], function_type = (!tt.ptr<f32>, !tt.ptr<i32>, !tt.ptr<f32>, i32) -> (), sym_name = "origin_index_select", sym_visibility = "public"}> ({
^bb0(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<i32>, %arg2: !tt.ptr<f32>, %arg3: i32):
  %0 = "arith.constant"() <{value = 1024 : index}> : () -> index
  %1 = "arith.constant"() <{value = 1 : index}> : () -> index
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index
  %3 = "arith.constant"() <{value = 1024 : i32}> : () -> i32
  %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32
  %5 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>
  %6 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>
  %7 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %8 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>
  %9 = "arith.constant"() <{value = 49299 : i32}> : () -> i32
  %10 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32
  %11 = "arith.muli"(%10, %9) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32
  %12 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>
  %13 = "tt.splat"(%11) : (i32) -> tensor<1024xi32>
  %14 = "arith.addi"(%12, %13) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
  %15 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>
  %16 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>
  %17 = "arith.cmpi"(%16, %6) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>
  %18 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi32>) -> tensor<1024x16xi32>
  %20 = "tt.expand_dims"(%17) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>
  %21 = "tt.broadcast"(%20) : (tensor<1x16xi1>) -> tensor<1024x16xi1>
  %22 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>
  "scf.for"(%4, %9, %3) ({
  ^bb0(%arg4: i32):
    %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>
    %24 = "arith.addi"(%14, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>
    %25 = "arith.cmpi"(%24, %8) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>
    %26 = "tt.addptr"(%15, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>
    %27 = "tt.load"(%26, %25, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>
    %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %29 = "arith.muli"(%28, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>
    %31 = "tt.broadcast"(%30) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>
    %32 = "arith.extsi"(%19) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>
    %33 = "arith.addi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>
    %34 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>
    %35 = "tt.broadcast"(%34) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>
    %36 = "arith.andi"(%35, %21) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>
    %37 = "tensor.empty"() : () -> tensor<1024xf32>
    %38 = "scf.for"(%2, %0, %1, %37) ({
    ^bb0(%arg5: index, %arg6: tensor<1024xf32>):
      %44 = "tensor.extract"(%33, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
      %45 = "tt.addptr"(%arg0, %44) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>
      %46 = "tt.load"(%45) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32
      %47 = "tt.splat"(%46) : (f32) -> tensor<16xf32>
      %48 = "tensor.insert_slice"(%47, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>
      "scf.yield"(%48) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()
    }) {ExtractedLoadOrStore} : (index, index, index, tensor<1024xf32>) -> tensor<1024xf32>
    %39 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>
    %40 = "arith.muli"(%39, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>
    %41 = "tt.addptr"(%22, %40) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>
    %42 = "tt.broadcast"(%41) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>
    %43 = "tt.addptr"(%42, %19) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>
    "tt.store"(%43, %38, %36) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()
    "scf.yield"() : () -> ()
  }) : (i32, i32, i32) -> ()
  "tt.return"() : () -> ()
}) {noinline = false} : () -> ()
KMLOG ptrOffsetInfo dump
1
"(anonymous namespace)::UnstructuredMemAccessConverter<mlir::triton::LoadOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366ad10) {
  %26 = "tt.addptr"(%15, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e908366ac20) {
  %25 = "arith.cmpi"(%24, %8) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e908366a670) {
  %24 = "arith.addi"(%14, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e908366d040) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e908366a100) {
  %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083669330) {
  %22 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e9083668dc0) {
  %21 = "tt.broadcast"(%20) : (tensor<1x16xi1>) -> tensor<1024x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083668cd0) {
  %20 = "tt.expand_dims"(%17) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836687a0) {
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi32>) -> tensor<1024x16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083649170) {
  %18 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e9083648b10) {
  %17 = "arith.cmpi"(%16, %6) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e9083666b90) {
  %16 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083666aa0) {
  %15 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e9083666510) {
  %14 = "arith.addi"(%12, %13) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083665b20) {
  %13 = "tt.splat"(%11) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e90836655d0) {
  %12 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e90836646f0) {
  %11 = "arith.muli"(%10, %9) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5e9083653f20) {
  %10 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836645e0) {
  %9 = "arith.constant"() <{value = 49299 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662c20) {
  %8 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083664400) {
  %7 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836635b0) {
  %6 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662730) {
  %5 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836613f0) {
  %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x5e908366d1b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083660580) {
  %3 = "arith.constant"() <{value = 1024 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::DominanceInfo)

//===-------------------------------------------===//
Processing operation : 'tt.func'(0x5e908366d1b0) {
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836a54d0) {
  %0 = "arith.constant"() <{value = 1024 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e908367b1c0) {
  %1 = "arith.constant"() <{value = 1 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836a5440) {
  %2 = "arith.constant"() <{value = 0 : index}> : () -> index

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083660580) {
  %3 = "arith.constant"() <{value = 1024 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836613f0) {
  %4 = "arith.constant"() <{value = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662730) {
  %5 = "arith.constant"() <{value = dense<16> : tensor<1024x1xi32>}> : () -> tensor<1024x1xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836635b0) {
  %6 = "arith.constant"() <{value = dense<16> : tensor<16xi32>}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083664400) {
  %7 = "arith.constant"() <{value = dense<0> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e9083662c20) {
  %8 = "arith.constant"() <{value = dense<1971940> : tensor<1024xi32>}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.constant'(0x5e90836645e0) {
  %9 = "arith.constant"() <{value = 49299 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.get_program_id'(0x5e9083653f20) {
  %10 = "tt.get_program_id"() <{axis = 0 : i32}> : () -> i32

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e90836646f0) {
  %11 = "arith.muli"(%10, %9) <{overflowFlags = #arith.overflow<none>}> : (i32, i32) -> i32


  * Pattern (anonymous namespace)::MulIMulIConstant : 'arith.muli -> (arith.constant, arith.muli)' {
Trying to match "(anonymous namespace)::MulIMulIConstant"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::MulIMulIConstant" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e90836655d0) {
  %12 = "tt.make_range"() <{end = 1024 : i32, start = 0 : i32}> : () -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083665b20) {
  %13 = "tt.splat"(%11) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e9083666510) {
  %14 = "arith.addi"(%12, %13) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Match Failure : castedOp1 is not ::mlir::arith::AddIOp type
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Match Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Match Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddIMulNegativeOneRhs : 'arith.addi -> (arith.subi)' {
Trying to match "(anonymous namespace)::AddIMulNegativeOneRhs"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::AddIMulNegativeOneRhs" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddIMulNegativeOneLhs : 'arith.addi -> (arith.subi)' {
Trying to match "(anonymous namespace)::AddIMulNegativeOneLhs"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::AddIMulNegativeOneLhs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083666aa0) {
  %15 = "tt.splat"(%arg1) : (!tt.ptr<i32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.make_range'(0x5e9083666b90) {
  %16 = "tt.make_range"() <{end = 16 : i32, start = 0 : i32}> : () -> tensor<16xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e9083648b10) {
  %17 = "arith.cmpi"(%16, %6) <{predicate = 2 : i64}> : (tensor<16xi32>, tensor<16xi32>) -> tensor<16xi1>


  * Pattern (anonymous namespace)::CmpIExtSI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "(anonymous namespace)::CmpIExtSI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtSIOp type
"(anonymous namespace)::CmpIExtSI" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::CmpIExtUI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "(anonymous namespace)::CmpIExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"(anonymous namespace)::CmpIExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083649170) {
  %18 = "tt.expand_dims"(%16) <{axis = 0 : i32}> : (tensor<16xi32>) -> tensor<1x16xi32>


  * Pattern  : 'tt.expand_dims -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836687a0) {
  %19 = "tt.broadcast"(%18) : (tensor<1x16xi32>) -> tensor<1024x16xi32>


  * Pattern  : 'tt.broadcast -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e9083668cd0) {
  %20 = "tt.expand_dims"(%17) <{axis = 0 : i32}> : (tensor<16xi1>) -> tensor<1x16xi1>


  * Pattern  : 'tt.expand_dims -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e9083668dc0) {
  %21 = "tt.broadcast"(%20) : (tensor<1x16xi1>) -> tensor<1024x16xi1>


  * Pattern  : 'tt.broadcast -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e9083669330) {
  %22 = "tt.splat"(%arg2) : (!tt.ptr<f32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e908366d040) {

  * Pattern (anonymous namespace)::ForOpIterArgsFolder : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::ForOpIterArgsFolder"
"(anonymous namespace)::ForOpIterArgsFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::SimplifyTrivialLoops : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::SimplifyTrivialLoops"
"(anonymous namespace)::SimplifyTrivialLoops" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ForOpTensorCastFolder : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::ForOpTensorCastFolder"
"(anonymous namespace)::ForOpTensorCastFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e908366a100) {
  %23 = "tt.splat"(%arg4) : (i32) -> tensor<1024xi32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e908366a670) {
  %24 = "arith.addi"(%14, %23) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi32>


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Match Failure : ::mlir::success(::mlir::matchPattern(op0->getResult(0), ::mlir::m_Constant(&arg1_0))) return ::mlir::failure
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Match Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Match Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddIMulNegativeOneRhs : 'arith.addi -> (arith.subi)' {
Trying to match "(anonymous namespace)::AddIMulNegativeOneRhs"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::AddIMulNegativeOneRhs" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddIMulNegativeOneLhs : 'arith.addi -> (arith.subi)' {
Trying to match "(anonymous namespace)::AddIMulNegativeOneLhs"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::AddIMulNegativeOneLhs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.cmpi'(0x5e908366ac20) {
  %25 = "arith.cmpi"(%24, %8) <{predicate = 2 : i64}> : (tensor<1024xi32>, tensor<1024xi32>) -> tensor<1024xi1>


  * Pattern (anonymous namespace)::CmpIExtSI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "(anonymous namespace)::CmpIExtSI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtSIOp type
"(anonymous namespace)::CmpIExtSI" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::CmpIExtUI : 'arith.cmpi -> (arith.cmpi)' {
Trying to match "(anonymous namespace)::CmpIExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"(anonymous namespace)::CmpIExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366ad10) {
  %26 = "tt.addptr"(%15, %24) : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi32>) -> tensor<1024x!tt.ptr<i32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e90836419a0) {
  %27 = "tt.load"(%26, %25, %7) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 1, 1>}> : (tensor<1024x!tt.ptr<i32>>, tensor<1024xi1>, tensor<1024xi32>) -> tensor<1024xi32>


  * Pattern mlir::triton::CanonicalizeMaskedLoadPattern : 'tt.load -> ()' {
Trying to match "mlir::triton::CanonicalizeMaskedLoadPattern"
"mlir::triton::CanonicalizeMaskedLoadPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366b780) {
  %28 = "tt.expand_dims"(%27) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>


  * Pattern  : 'tt.expand_dims -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366bcf0) {
  %29 = "arith.muli"(%28, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>


  * Pattern (anonymous namespace)::MulIMulIConstant : 'arith.muli -> (arith.constant, arith.muli)' {
Trying to match "(anonymous namespace)::MulIMulIConstant"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::MulIMulIConstant" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836acd70) {
  %30 = "arith.extsi"(%29) : (tensor<1024x1xi32>) -> tensor<1024x1xi64>


  * Pattern (anonymous namespace)::ExtSIOfExtUI : 'arith.extsi -> (arith.extui)' {
Trying to match "(anonymous namespace)::ExtSIOfExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"(anonymous namespace)::ExtSIOfExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e90836a5bb0) {
  %31 = "tt.broadcast"(%30) : (tensor<1024x1xi64>) -> tensor<1024x16xi64>


  * Pattern  : 'tt.broadcast -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.extsi'(0x5e90836ad560) {
  %32 = "arith.extsi"(%19) : (tensor<1024x16xi32>) -> tensor<1024x16xi64>


  * Pattern (anonymous namespace)::ExtSIOfExtUI : 'arith.extsi -> (arith.extui)' {
Trying to match "(anonymous namespace)::ExtSIOfExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"(anonymous namespace)::ExtSIOfExtUI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.addi'(0x5e90836accc0) {
  %33 = "arith.addi"(%31, %32) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x16xi64>, tensor<1024x16xi64>) -> tensor<1024x16xi64>


  * Pattern (anonymous namespace)::AddIAddConstant : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddIAddConstant"
    ** Match Failure : castedOp1 is not ::mlir::arith::AddIOp type
"(anonymous namespace)::AddIAddConstant" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantRHS : 'arith.addi -> (arith.addi, arith.constant)' {
Trying to match "(anonymous namespace)::AddISubConstantRHS"
    ** Match Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantRHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddISubConstantLHS : 'arith.addi -> (arith.constant, arith.subi)' {
Trying to match "(anonymous namespace)::AddISubConstantLHS"
    ** Match Failure : castedOp1 is not ::mlir::arith::SubIOp type
"(anonymous namespace)::AddISubConstantLHS" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddIMulNegativeOneRhs : 'arith.addi -> (arith.subi)' {
Trying to match "(anonymous namespace)::AddIMulNegativeOneRhs"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::AddIMulNegativeOneRhs" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AddIMulNegativeOneLhs : 'arith.addi -> (arith.subi)' {
Trying to match "(anonymous namespace)::AddIMulNegativeOneLhs"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::AddIMulNegativeOneLhs" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366c110) {
  %34 = "tt.expand_dims"(%25) <{axis = 1 : i32}> : (tensor<1024xi1>) -> tensor<1024x1xi1>


  * Pattern  : 'tt.expand_dims -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366c200) {
  %35 = "tt.broadcast"(%34) : (tensor<1024x1xi1>) -> tensor<1024x16xi1>


  * Pattern  : 'tt.broadcast -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.andi'(0x5e9083669fb0) {
  %36 = "arith.andi"(%35, %21) : (tensor<1024x16xi1>, tensor<1024x16xi1>) -> tensor<1024x16xi1>


  * Pattern (anonymous namespace)::AndOfExtUI : 'arith.andi -> (arith.andi, arith.extui)' {
Trying to match "(anonymous namespace)::AndOfExtUI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtUIOp type
"(anonymous namespace)::AndOfExtUI" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::AndOfExtSI : 'arith.andi -> (arith.andi, arith.extsi)' {
Trying to match "(anonymous namespace)::AndOfExtSI"
    ** Match Failure : castedOp1 is not ::mlir::arith::ExtSIOp type
"(anonymous namespace)::AndOfExtSI" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.empty'(0x5e90836aef70) {
  %37 = "tensor.empty"() : () -> tensor<1024xf32>


  * Pattern (anonymous namespace)::ReplaceEmptyTensorStaticShapeDims : 'tensor.empty -> ()' {
Trying to match "(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims"
"(anonymous namespace)::ReplaceEmptyTensorStaticShapeDims" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.for'(0x5e90836af8d0) {

  * Pattern (anonymous namespace)::ForOpIterArgsFolder : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::ForOpIterArgsFolder"
"(anonymous namespace)::ForOpIterArgsFolder" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::SimplifyTrivialLoops : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::SimplifyTrivialLoops"
"(anonymous namespace)::SimplifyTrivialLoops" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ForOpTensorCastFolder : 'scf.for -> ()' {
Trying to match "(anonymous namespace)::ForOpTensorCastFolder"
"(anonymous namespace)::ForOpTensorCastFolder" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.extract'(0x5e90836a65c0) {
  %44 = "tensor.extract"(%33, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64


  * Pattern (anonymous namespace)::FoldFillWithTensorExtract : 'tensor.extract -> ()' {
Trying to match "(anonymous namespace)::FoldFillWithTensorExtract"
"(anonymous namespace)::FoldFillWithTensorExtract" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ExtractFromTensorGenerate : 'tensor.extract -> ()' {
Trying to match "(anonymous namespace)::ExtractFromTensorGenerate"
"(anonymous namespace)::ExtractFromTensorGenerate" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ExtractElementFromIndexCast : 'tensor.extract -> ()' {
Trying to match "(anonymous namespace)::ExtractElementFromIndexCast"
"(anonymous namespace)::ExtractElementFromIndexCast" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::ExtractFromTensorCast : 'tensor.extract -> ()' {
Trying to match "(anonymous namespace)::ExtractFromTensorCast"
"(anonymous namespace)::ExtractFromTensorCast" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e90836adab0) {
  %45 = "tt.addptr"(%arg0, %44) : (!tt.ptr<f32>, i64) -> !tt.ptr<f32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.load'(0x5e90836aa490) {
  %46 = "tt.load"(%45) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32, isVolatile = false, operandSegmentSizes = array<i32: 1, 0, 0>}> {DiscreteMemAccess} : (!tt.ptr<f32>) -> f32


  * Pattern mlir::triton::CanonicalizeMaskedLoadPattern : 'tt.load -> ()' {
Trying to match "mlir::triton::CanonicalizeMaskedLoadPattern"
"mlir::triton::CanonicalizeMaskedLoadPattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.splat'(0x5e90836a64e0) {
  %47 = "tt.splat"(%46) : (f32) -> tensor<16xf32>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tensor.insert_slice'(0x5e90836ae9a0) {
  %48 = "tensor.insert_slice"(%47, %arg6, %arg5) <{operandSegmentSizes = array<i32: 1, 1, 1, 0, 0>, static_offsets = array<i64: -9223372036854775808>, static_sizes = array<i64: 1>, static_strides = array<i64: 1>}> : (tensor<16xf32>, tensor<1024xf32>, index) -> tensor<1024xf32>


  * Pattern FoldTensorCastProducerOp : 'tensor.insert_slice -> ()' {
Trying to match "FoldTensorCastProducerOp"
"FoldTensorCastProducerOp" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::FoldInsertPadIntoFill : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::FoldInsertPadIntoFill"
"(anonymous namespace)::FoldInsertPadIntoFill" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpConstantArgumentFolder<mlir::tensor::InsertSliceOp> : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpConstantArgumentFolder<mlir::tensor::InsertSliceOp>"
"(anonymous namespace)::InsertSliceOpConstantArgumentFolder<mlir::tensor::InsertSliceOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpCastFolder<mlir::tensor::InsertSliceOp> : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpCastFolder<mlir::tensor::InsertSliceOp>"
"(anonymous namespace)::InsertSliceOpCastFolder<mlir::tensor::InsertSliceOp>" result 0
  } -> failure : pattern failed to match

  * Pattern (anonymous namespace)::InsertSliceOpSourceCastInserter<mlir::tensor::InsertSliceOp> : 'tensor.insert_slice -> ()' {
Trying to match "(anonymous namespace)::InsertSliceOpSourceCastInserter<mlir::tensor::InsertSliceOp>"
"(anonymous namespace)::InsertSliceOpSourceCastInserter<mlir::tensor::InsertSliceOp>" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x5e90836aceb0) {
  "scf.yield"(%48) {DiscreteMemAccess} : (tensor<1024xf32>) -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.expand_dims'(0x5e908366cb40) {
  %39 = "tt.expand_dims"(%24) <{axis = 1 : i32}> : (tensor<1024xi32>) -> tensor<1024x1xi32>


  * Pattern  : 'tt.expand_dims -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'arith.muli'(0x5e908366cc30) {
  %40 = "arith.muli"(%39, %5) <{overflowFlags = #arith.overflow<none>}> : (tensor<1024x1xi32>, tensor<1024x1xi32>) -> tensor<1024x1xi32>


  * Pattern (anonymous namespace)::MulIMulIConstant : 'arith.muli -> (arith.constant, arith.muli)' {
Trying to match "(anonymous namespace)::MulIMulIConstant"
    ** Match Failure : castedOp1 is not ::mlir::arith::MulIOp type
"(anonymous namespace)::MulIMulIConstant" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cd40) {
  %41 = "tt.addptr"(%22, %40) : (tensor<1024x1x!tt.ptr<f32>>, tensor<1024x1xi32>) -> tensor<1024x1x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.broadcast'(0x5e908366ce50) {
  %42 = "tt.broadcast"(%41) : (tensor<1024x1x!tt.ptr<f32>>) -> tensor<1024x16x!tt.ptr<f32>>


  * Pattern  : 'tt.broadcast -> ()' {
Trying to match ""
"" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.addptr'(0x5e908366cf40) {
  %43 = "tt.addptr"(%42, %19) : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024x16xi32>) -> tensor<1024x16x!tt.ptr<f32>>

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.store'(0x5e9083664460) {
  "tt.store"(%43, %38, %36) <{boundaryCheck = array<i32>, cache = 1 : i32, evict = 1 : i32}> : (tensor<1024x16x!tt.ptr<f32>>, tensor<1024xf32>, tensor<1024x16xi1>) -> ()


  * Pattern mlir::triton::CanonicalizeMaskedStorePattern : 'tt.store -> ()' {
Trying to match "mlir::triton::CanonicalizeMaskedStorePattern"
"mlir::triton::CanonicalizeMaskedStorePattern" result 0
  } -> failure : pattern failed to match
} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'scf.yield'(0x5e9083646c30) {
  "scf.yield"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//

//===-------------------------------------------===//
Processing operation : 'tt.return'(0x5e9083650000) {
  "tt.return"() : () -> ()

} -> failure : pattern failed to match
//===-------------------------------------------===//
ImplicitTypeIDRegistry::lookupOrInsert(mlir::TypedAttr::Trait<Empty>)
/home/devro/workspace/ts_sandbox/test_loop_index_select.py:40:87: error: 'tensor.extract' op incorrect number of indices for extract_element
/home/devro/workspace/ts_sandbox/test_loop_index_select.py:40:87: note: see current operation: %44 = "tensor.extract"(%33, %arg5) {DiscreteMemAccess} : (tensor<1024x16xi64>, index) -> i64
